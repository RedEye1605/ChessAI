"""
================================================================================
üèÜ COMPLETE CHESS RL TRAINING SCRIPT
================================================================================
Script lengkap untuk training AI Catur dengan PPO.
Dapat dipindahkan ke Colab Notebook dengan memisahkan berdasarkan "# Cell X"

Fix utama:
- Menggunakan Categorical(logits=...) instead of Categorical(probs=...)
- NaN detection dan recovery
- Improved numerical stability

Author: Generated by AI Assistant
Date: 2026-01-14
================================================================================
"""

# ==============================================================================
# Cell 1: Setup & GPU Check (Optimized for Kaggle T4x2)
# ==============================================================================

import torch
import numpy as np
import random
import os
import sys
import time
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # Enable benchmark mode for faster training (disable for reproducibility)
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True  # Faster convolutions

set_seed(42)

# Multi-GPU and Device setup
N_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0
USE_MULTI_GPU = N_GPUS > 1
USE_FP16 = torch.cuda.is_available()  # Mixed precision training

if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"‚úÖ Found {N_GPUS} GPU(s):")
    total_mem = 0
    for i in range(N_GPUS):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_mem = torch.cuda.get_device_properties(i).total_memory / 1e9
        total_mem += gpu_mem
        print(f"   GPU {i}: {gpu_name} ({gpu_mem:.1f} GB)")
    print(f"   Total VRAM: {total_mem:.1f} GB")
    if USE_MULTI_GPU:
        print(f"‚úÖ Multi-GPU mode: DataParallel on {N_GPUS} GPUs")
    if USE_FP16:
        print(f"‚úÖ Mixed Precision (FP16) enabled")
else:
    device = torch.device('cpu')
    print("‚ö†Ô∏è GPU tidak tersedia! Training akan sangat lambat.")

print(f"‚úÖ Device: {device}")
print(f"‚úÖ PyTorch version: {torch.__version__}")

# ==============================================================================
# Cell 2: Install Dependencies
# ==============================================================================

# Uncomment these lines when running in Colab:
# !pip install -q python-chess gymnasium tqdm matplotlib
# !apt-get install -qq stockfish > /dev/null 2>&1
# print("‚úÖ Dependencies installed!")

# For local testing, just import (assuming already installed)
try:
    import chess
    import gymnasium as gym
    from gymnasium import spaces
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    print("‚úÖ All dependencies loaded!")
except ImportError as e:
    print(f"‚ùå Missing dependency: {e}")
    print("   Run: pip install python-chess gymnasium tqdm matplotlib")

# ==============================================================================
# Cell 3: Chess Environment with Enhanced State Encoding
# ==============================================================================

class ChessEnv(gym.Env):
    """
    Enhanced Chess Environment dengan reward shaping dan numerical stability.
    
    State: 18 channels x 8 x 8
    - 12 piece planes (6 white + 6 black)
    - 1 turn indicator
    - 4 castling rights
    - 1 en passant
    
    Action: 4672 possible moves (AlphaZero encoding)
    """
    
    PIECE_VALUES = {
        chess.PAWN: 1,
        chess.KNIGHT: 3,
        chess.BISHOP: 3,
        chess.ROOK: 5,
        chess.QUEEN: 9,
        chess.KING: 0
    }
    
    def __init__(self, max_moves=200, reward_shaping=True):
        super().__init__()
        self.board = chess.Board()
        self.max_moves = max_moves
        self.move_count = 0
        self.reward_shaping = reward_shaping
        self.prev_material = 0
        
        self.observation_space = spaces.Box(
            low=0, high=1, shape=(18, 8, 8), dtype=np.float32
        )
        self.action_space = spaces.Discrete(4672)
        self._init_move_encoding()
    
    def _init_move_encoding(self):
        """Initialize action space dengan AlphaZero-style encoding."""
        self.action_to_move = {}
        self.move_to_action = {}
        
        # Direction vectors untuk queen-like moves
        directions = []
        for d in [(0,1), (0,-1), (1,0), (-1,0), (1,1), (1,-1), (-1,1), (-1,-1)]:
            for dist in range(1, 8):
                directions.append((d[0]*dist, d[1]*dist))
        
        # Knight moves
        for d in [(1,2), (2,1), (2,-1), (1,-2), (-1,-2), (-2,-1), (-2,1), (-1,2)]:
            directions.append(d)
        
        action = 0
        for sq in range(64):
            from_rank, from_file = sq // 8, sq % 8
            
            for dx, dy in directions:
                to_rank = from_rank + dy
                to_file = from_file + dx
                
                if 0 <= to_rank < 8 and 0 <= to_file < 8:
                    to_sq = to_rank * 8 + to_file
                    move = chess.Move(sq, to_sq)
                    self.action_to_move[action] = move
                    self.move_to_action[move.uci()] = action
                action += 1
            
            # Underpromotions
            if from_rank == 6:
                for dx in [-1, 0, 1]:
                    for promo in [chess.KNIGHT, chess.BISHOP, chess.ROOK]:
                        to_file = from_file + dx
                        if 0 <= to_file < 8:
                            to_sq = 7 * 8 + to_file
                            move = chess.Move(sq, to_sq, promotion=promo)
                            self.action_to_move[action] = move
                            self.move_to_action[move.uci()] = action
                        action += 1
    
    def _get_material_score(self, color):
        """Calculate material score for given color."""
        score = 0
        for piece_type in self.PIECE_VALUES:
            score += len(self.board.pieces(piece_type, color)) * self.PIECE_VALUES[piece_type]
        return score
    
    def encode_state(self):
        """Encode board state sebagai numpy array."""
        state = np.zeros((18, 8, 8), dtype=np.float32)
        
        piece_to_channel = {
            (chess.PAWN, True): 0, (chess.KNIGHT, True): 1, (chess.BISHOP, True): 2,
            (chess.ROOK, True): 3, (chess.QUEEN, True): 4, (chess.KING, True): 5,
            (chess.PAWN, False): 6, (chess.KNIGHT, False): 7, (chess.BISHOP, False): 8,
            (chess.ROOK, False): 9, (chess.QUEEN, False): 10, (chess.KING, False): 11
        }
        
        for sq in chess.SQUARES:
            piece = self.board.piece_at(sq)
            if piece:
                rank, file = sq // 8, sq % 8
                ch = piece_to_channel[(piece.piece_type, piece.color)]
                state[ch, rank, file] = 1.0
        
        # Turn
        state[12, :, :] = 1.0 if self.board.turn else 0.0
        
        # Castling rights
        state[13, 0, :] = float(self.board.has_kingside_castling_rights(True))
        state[14, 0, :] = float(self.board.has_queenside_castling_rights(True))
        state[15, 0, :] = float(self.board.has_kingside_castling_rights(False))
        state[16, 0, :] = float(self.board.has_queenside_castling_rights(False))
        
        # En passant
        if self.board.ep_square:
            ep_rank, ep_file = self.board.ep_square // 8, self.board.ep_square % 8
            state[17, ep_rank, ep_file] = 1.0
        
        return state
    
    def get_legal_action_mask(self):
        """Get binary mask untuk legal actions."""
        mask = np.zeros(4672, dtype=bool)
        for move in self.board.legal_moves:
            uci = move.uci()
            if uci in self.move_to_action:
                mask[self.move_to_action[uci]] = True
            # Handle promotions (default to queen)
            elif len(uci) == 5 and uci[4] == 'q':
                base_uci = uci[:4]
                if base_uci in self.move_to_action:
                    mask[self.move_to_action[base_uci]] = True
        return mask
    
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.board = chess.Board()
        self.move_count = 0
        white_material = self._get_material_score(True)
        black_material = self._get_material_score(False)
        self.prev_material = white_material - black_material
        return self.encode_state(), {}
    
    def step(self, action):
        """Execute action, return (state, reward, terminated, truncated, info)."""
        if action in self.action_to_move:
            move = self.action_to_move[action]
            
            # Try exact match first
            if move in self.board.legal_moves:
                self.board.push(move)
                self.move_count += 1
            else:
                # Try to find promotion variant
                found = False
                for legal in self.board.legal_moves:
                    if legal.uci()[:4] == move.uci()[:4]:
                        self.board.push(legal)
                        self.move_count += 1
                        found = True
                        break
                
                if not found:
                    return self.encode_state(), -1.0, True, False, {'illegal': True}
        else:
            return self.encode_state(), -1.0, True, False, {'illegal': True}
        
        reward = 0.0
        terminated = False
        truncated = False
        
        if self.board.is_checkmate():
            # We just moved, so if it's checkmate, we won
            reward = 1.0
            terminated = True
        elif self.board.is_game_over():
            # Draw
            reward = 0.0
            terminated = True
        elif self.move_count >= self.max_moves:
            truncated = True
        elif self.reward_shaping:
            # Reward shaping based on material
            white_material = self._get_material_score(True)
            black_material = self._get_material_score(False)
            current_material = white_material - black_material
            material_diff = current_material - self.prev_material
            self.prev_material = current_material
            
            # ENHANCED reward shaping (V2)
            if material_diff != 0:
                reward = material_diff * 0.05  # 0.01 ‚Üí 0.05 (5x stronger)
            
            if self.board.is_check():
                reward += 0.02  # 0.005 ‚Üí 0.02 (4x stronger)
        
        return self.encode_state(), reward, terminated, truncated, {}

print("‚úÖ ChessEnv defined!")

# ==============================================================================
# Cell 3.5: Curriculum Environment with Opponent Selection
# ==============================================================================

try:
    from stockfish import Stockfish
    STOCKFISH_PATH = "/usr/games/stockfish"  # Kaggle path
    STOCKFISH_AVAILABLE = True
except ImportError:
    STOCKFISH_AVAILABLE = False
    print("‚ö†Ô∏è Stockfish not available, will use Random opponent only")

class CurriculumChessEnv(ChessEnv):
    """
    Chess Environment with curriculum learning support.
    Supports different opponents: random, self, stockfish_l0, stockfish_l1, etc.
    """
    
    OPPONENT_TYPES = ['random', 'self', 'stockfish_l0', 'stockfish_l1', 'stockfish_l2']
    
    def __init__(self, opponent='random', stockfish_time=0.01, **kwargs):
        super().__init__(**kwargs)
        self.opponent_type = opponent
        self.stockfish_time = stockfish_time
        self.stockfish = None
        
        # Initialize Stockfish if needed
        if 'stockfish' in opponent and STOCKFISH_AVAILABLE:
            try:
                self.stockfish = Stockfish(STOCKFISH_PATH)
                level = int(opponent.split('_l')[1]) if '_l' in opponent else 0
                self.stockfish.set_skill_level(level)
                self.stockfish.set_depth(5)  # Limit depth for speed
            except Exception as e:
                print(f"‚ö†Ô∏è Stockfish init failed: {e}")
                self.opponent_type = 'random'
    
    def set_opponent(self, opponent_type):
        """Change opponent type during training."""
        self.opponent_type = opponent_type
        if 'stockfish' in opponent_type and self.stockfish:
            level = int(opponent_type.split('_l')[1]) if '_l' in opponent_type else 0
            self.stockfish.set_skill_level(level)
    
    def get_opponent_move(self):
        """Get move from opponent based on opponent_type."""
        legal_moves = list(self.board.legal_moves)
        if not legal_moves:
            return None
        
        if self.opponent_type == 'random':
            return np.random.choice(legal_moves)
        
        elif self.opponent_type == 'self':
            # Self-play: return None, let the agent play both sides
            return None
        
        elif 'stockfish' in self.opponent_type and self.stockfish:
            try:
                self.stockfish.set_fen_position(self.board.fen())
                best_move = self.stockfish.get_best_move_time(int(self.stockfish_time * 1000))
                if best_move:
                    return chess.Move.from_uci(best_move)
            except Exception:
                pass
            return np.random.choice(legal_moves)  # Fallback to random
        
        else:
            return np.random.choice(legal_moves)
    
    def step(self, action):
        """Execute agent action, then opponent action if applicable."""
        # Agent's move
        state, reward, terminated, truncated, info = super().step(action)
        
        if terminated or truncated:
            return state, reward, terminated, truncated, info
        
        # Opponent's move (if not self-play)
        if self.opponent_type != 'self':
            opp_move = self.get_opponent_move()
            if opp_move and opp_move in self.board.legal_moves:
                self.board.push(opp_move)
                self.move_count += 1
                
                # Check game end after opponent move
                if self.board.is_checkmate():
                    # Opponent checkmated us
                    reward = -1.0
                    terminated = True
                elif self.board.is_game_over():
                    reward = 0.0
                    terminated = True
                elif self.move_count >= self.max_moves:
                    truncated = True
                else:
                    # Opponent's capture hurts us
                    if self.reward_shaping:
                        white_material = self._get_material_score(True)
                        black_material = self._get_material_score(False)
                        current_material = white_material - black_material
                        material_diff = current_material - self.prev_material
                        self.prev_material = current_material
                        if material_diff != 0:
                            reward += material_diff * 0.05
                
                state = self.encode_state()
        
        return state, reward, terminated, truncated, info

print("‚úÖ CurriculumChessEnv defined!")

# ==============================================================================
# Cell 4: Neural Network (Residual + SE Blocks)
# ==============================================================================

import torch.nn as nn
import torch.nn.functional as F

class SEBlock(nn.Module):
    """Squeeze-and-Excitation block untuk channel attention."""
    
    def __init__(self, channels, reduction=4):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)
    
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.pool(x).view(b, c)
        y = F.relu(self.fc1(y))
        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)
        return x * y

class ResidualBlock(nn.Module):
    """Residual block dengan optional SE attention."""
    
    def __init__(self, channels, use_se=True):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(channels)
        self.se = SEBlock(channels) if use_se else nn.Identity()
    
    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.se(out)
        return F.relu(out + residual)

class ChessNetwork(nn.Module):
    """
    Policy-Value Network untuk Chess RL.
    
    Architecture:
    - Input: (batch, 18, 8, 8)
    - Residual backbone dengan SE blocks
    - Policy head ‚Üí log probabilities (batch, 4672)
    - Value head ‚Üí value estimate (batch, 1)
    """
    
    def __init__(self, input_channels=18, num_filters=256, num_blocks=12, action_size=4672):
        super().__init__()
        
        self.action_size = action_size
        
        # Input conv
        self.input_conv = nn.Sequential(
            nn.Conv2d(input_channels, num_filters, 3, padding=1, bias=False),
            nn.BatchNorm2d(num_filters),
            nn.ReLU(inplace=True)
        )
        
        # Residual tower dengan SE blocks (every other block)
        self.res_blocks = nn.ModuleList([
            ResidualBlock(num_filters, use_se=(i % 2 == 0))
            for i in range(num_blocks)
        ])
        
        # Policy head
        self.policy_conv = nn.Conv2d(num_filters, 80, 1, bias=False)
        self.policy_bn = nn.BatchNorm2d(80)
        self.policy_fc = nn.Linear(80 * 64, action_size)
        
        # Value head
        self.value_conv = nn.Conv2d(num_filters, 32, 1, bias=False)
        self.value_bn = nn.BatchNorm2d(32)
        self.value_fc1 = nn.Linear(32 * 64, 256)
        self.value_fc2 = nn.Linear(256, 1)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights dengan Kaiming initialization."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
        
        # Smaller initialization untuk policy output (stability)
        nn.init.xavier_uniform_(self.policy_fc.weight, gain=0.01)
        nn.init.zeros_(self.policy_fc.bias)
        nn.init.xavier_uniform_(self.value_fc2.weight, gain=0.01)
        nn.init.zeros_(self.value_fc2.bias)
    
    def forward(self, x, legal_mask=None):
        """
        Forward pass.
        
        Args:
            x: Input tensor (batch, 18, 8, 8)
            legal_mask: Boolean mask for legal actions (batch, 4672)
            
        Returns:
            log_probs: Log probabilities (batch, action_size)
            value: Value estimate (batch, 1)
        """
        # Backbone
        x = self.input_conv(x)
        for block in self.res_blocks:
            x = block(x)
        
        # Policy head
        policy = F.relu(self.policy_bn(self.policy_conv(x)))
        policy = policy.view(policy.size(0), -1)
        policy_logits = self.policy_fc(policy)
        
        # Apply legal action mask
        # NOTE: Use -1e4 instead of -1e9 for FP16 compatibility (Half max is ~65504)
        if legal_mask is not None:
            # Cast to float32 for masking, then back for softmax
            policy_logits = policy_logits.float()
            policy_logits = policy_logits.masked_fill(~legal_mask, -1e4)
        
        # Log softmax dengan numerical stability
        log_probs = F.log_softmax(policy_logits, dim=-1)
        
        # Value head
        value = F.relu(self.value_bn(self.value_conv(x)))
        value = value.view(value.size(0), -1)
        value = F.relu(self.value_fc1(value))
        value = torch.tanh(self.value_fc2(value))
        
        return log_probs, value

print("‚úÖ ChessNetwork defined!")

# ==============================================================================
# Cell 5: Configuration (V3 - Curriculum Learning for 1000 ELO)
# ==============================================================================

# P100 has 16GB VRAM - can use large batch sizes
BATCH_SIZE = 512

CONFIG = {
    # Network
    'input_channels': 18,
    'num_filters': 256,
    'num_blocks': 12,
    
    # PPO Hyperparameters (V3 - Stable)
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'clip_range': 0.1,
    'entropy_coef': 0.05,         # High entropy to prevent collapse
    'value_coef': 0.5,
    'max_grad_norm': 0.5,
    
    # Training
    'learning_rate': 1e-4,
    'weight_decay': 1e-4,
    'n_steps': 2048,
    'n_epochs': 3,
    'batch_size': BATCH_SIZE,
    'total_updates': 5000,        # Per phase
    
    # Checkpointing & Logging
    'save_interval': 500,
    'eval_interval': 250,
    'log_interval': 50,
    
    # Stability
    'min_entropy': 0.5,
    'max_kl': 0.02,
    'nan_recovery': True,
    
    # Performance
    'use_fp16': USE_FP16,
    
    # CURRICULUM LEARNING (V3 - NEW!)
    'curriculum_phases': [
        {'name': 'Phase 1: vs Random', 'opponent': 'random', 'updates': 2000, 'target_winrate': 0.70},
        {'name': 'Phase 2: vs Stockfish L0', 'opponent': 'stockfish_l0', 'updates': 2000, 'target_winrate': 0.30},
        {'name': 'Phase 3: vs Stockfish L1', 'opponent': 'stockfish_l1', 'updates': 1000, 'target_winrate': 0.20},
    ],
    'phase_transition_winrate': 0.6,  # Need 60% win to advance phase
}

print("üìã Configuration:")
for k, v in CONFIG.items():
    print(f"   {k}: {v}")

# ==============================================================================
# Cell 6: Create Network & Optimizer (V3 - Curriculum Learning Ready)
# ==============================================================================

# NOTE: Using CurriculumChessEnv for curriculum learning with opponents
# Bottleneck is environment stepping, not network forward pass.

# Number of parallel environments
NUM_ENVS = 16

# Start with Phase 1 opponent (Random)
current_phase = 0
current_opponent = CONFIG['curriculum_phases'][0]['opponent']

# Create curriculum environments
envs = [CurriculumChessEnv(
    opponent=current_opponent, 
    max_moves=200, 
    reward_shaping=True
) for _ in range(NUM_ENVS)]
env = envs[0]

print(f"‚úÖ Created {NUM_ENVS} CurriculumChessEnv")
print(f"   Starting opponent: {current_opponent}")
print(f"   Observation: {env.observation_space.shape}, Actions: {env.action_space.n}")

# Create network (single GPU is more efficient for RL)
network = ChessNetwork(
    input_channels=CONFIG['input_channels'],
    num_filters=CONFIG['num_filters'],
    num_blocks=CONFIG['num_blocks'],
    action_size=4672
).to(device)

# NOTE: torch.compile requires CUDA >= 7.0 (Triton), P100 is CUDA 6.0
# Skipping torch.compile for compatibility with older GPUs
# (If using newer GPU like T4/V100/A100, you can uncomment)
# if hasattr(torch, 'compile') and torch.cuda.is_available():
#     network = torch.compile(network, mode='reduce-overhead')
print("‚ÑπÔ∏è torch.compile disabled (P100 uses CUDA 6.0, Triton needs >= 7.0)")

num_params = sum(p.numel() for p in network.parameters())
print(f"‚úÖ Network: {num_params:,} parameters")

# Helper to get underlying model (handles compiled model)
def get_model():
    """Get the actual model, handling torch.compile if used."""
    if hasattr(network, '_orig_mod'):
        return network._orig_mod
    return network

# Optimizer dengan weight decay
optimizer = torch.optim.AdamW(
    network.parameters(),
    lr=CONFIG['learning_rate'],
    weight_decay=CONFIG['weight_decay'],
    betas=(0.9, 0.999),
    eps=1e-8
)

# Learning rate scheduler 
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=CONFIG['total_updates'],
    eta_min=1e-6
)

# Mixed Precision GradScaler for FP16
scaler = torch.cuda.amp.GradScaler(enabled=CONFIG['use_fp16'])

print("‚úÖ Optimizer: AdamW with cosine annealing LR")
if CONFIG['use_fp16']:
    print("‚úÖ GradScaler enabled for mixed precision (FP16)")

# ==============================================================================
# Cell 7: Training Functions (FIXED - No NaN + Batch Inference)
# ==============================================================================

from torch.distributions import Categorical

def select_action(state, legal_mask):
    """
    Select action untuk single state (untuk evaluation).
    """
    network.eval()
    with torch.no_grad():
        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
        mask_t = torch.BoolTensor(legal_mask).unsqueeze(0).to(device)
        
        with torch.cuda.amp.autocast(enabled=CONFIG['use_fp16']):
            log_probs, value = network(state_t, mask_t)
        
        if mask_t.sum() == 0:
            legal_actions = np.where(legal_mask)[0]
            action = np.random.choice(legal_actions) if len(legal_actions) > 0 else 0
            return action, 0.0, value.item()
        
        dist = Categorical(logits=log_probs)
        action = dist.sample()
        action_log_prob = log_probs[0, action.item()].item()
        
    network.train()
    return action.item(), action_log_prob, value.item()

def select_actions_batch(states, legal_masks):
    """
    BATCH inference: Select actions untuk semua environments dalam SATU forward pass.
    Ini JAUH lebih efisien daripada memanggil select_action berkali-kali.
    
    Args:
        states: List of states dari semua envs
        legal_masks: List of legal masks dari semua envs
        
    Returns:
        actions, log_probs, values (lists)
    """
    batch_size = len(states)
    
    network.eval()
    with torch.no_grad():
        # Stack semua states dan masks menjadi batch tensor
        states_t = torch.FloatTensor(np.array(states)).to(device)
        masks_t = torch.BoolTensor(np.array(legal_masks)).to(device)
        
        # SATU forward pass untuk semua envs!
        with torch.cuda.amp.autocast(enabled=CONFIG['use_fp16']):
            log_probs_batch, values_batch = network(states_t, masks_t)
        
        # Sample actions for each env
        actions = []
        action_log_probs = []
        
        for i in range(batch_size):
            if masks_t[i].sum() == 0:
                # Fallback
                legal_actions = np.where(legal_masks[i])[0]
                action = np.random.choice(legal_actions) if len(legal_actions) > 0 else 0
                actions.append(action)
                action_log_probs.append(0.0)
            else:
                dist = Categorical(logits=log_probs_batch[i:i+1])
                action = dist.sample()
                actions.append(action.item())
                action_log_probs.append(log_probs_batch[i, action.item()].item())
        
        values = values_batch.squeeze(-1).cpu().numpy().tolist()
    
    network.train()
    return actions, action_log_probs, values

def compute_gae(rewards, values, dones, last_value):
    """Compute Generalized Advantage Estimation."""
    gamma = CONFIG['gamma']
    lam = CONFIG['gae_lambda']
    advantages = np.zeros_like(rewards, dtype=np.float32)
    last_gae = 0
    
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = last_value
        else:
            next_value = values[t + 1]
        
        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
        advantages[t] = last_gae = delta + gamma * lam * (1 - dones[t]) * last_gae
    
    returns = advantages + np.array(values, dtype=np.float32)
    return advantages, returns

def check_for_nan(tensor, name="tensor"):
    """Check dan handle NaN values."""
    if torch.isnan(tensor).any() or torch.isinf(tensor).any():
        print(f"‚ö†Ô∏è NaN/Inf detected in {name}!")
        return True
    return False

print("‚úÖ Training functions defined!")

# ==============================================================================
# Cell 8: PPO Update Function
# ==============================================================================

def ppo_update(states, actions, old_log_probs, advantages, returns, masks):
    """
    Perform PPO update with mixed precision (FP16) and stability checks.
    
    Returns:
        dict: Statistics (policy_loss, value_loss, entropy, kl)
    """
    # Convert to tensors
    states_t = torch.FloatTensor(np.array(states)).to(device)
    actions_t = torch.LongTensor(actions).to(device)
    old_log_probs_t = torch.FloatTensor(old_log_probs).to(device)
    advantages_t = torch.FloatTensor(advantages).to(device)
    returns_t = torch.FloatTensor(returns).to(device)
    masks_t = torch.BoolTensor(np.array(masks)).to(device)
    
    # Normalize advantages
    advantages_t = (advantages_t - advantages_t.mean()) / (advantages_t.std() + 1e-8)
    
    all_policy_loss = []
    all_value_loss = []
    all_entropy = []
    all_kl = []
    
    dataset_size = len(states)
    
    for epoch in range(CONFIG['n_epochs']):
        indices = np.random.permutation(dataset_size)
        
        for start in range(0, dataset_size, CONFIG['batch_size']):
            end = min(start + CONFIG['batch_size'], dataset_size)
            batch_idx = indices[start:end]
            
            # Mixed precision forward pass
            with torch.cuda.amp.autocast(enabled=CONFIG['use_fp16']):
                # Forward pass
                log_probs, values_pred = network(states_t[batch_idx], masks_t[batch_idx])
                values_pred = values_pred.squeeze(-1)
                
                # Check for NaN
                if check_for_nan(log_probs, "log_probs"):
                    continue
                if check_for_nan(values_pred, "values_pred"):
                    continue
                
                # Get log probs for selected actions
                action_log_probs = log_probs.gather(1, actions_t[batch_idx].unsqueeze(-1)).squeeze(-1)
                
                # Policy loss (PPO clipped objective)
                ratio = torch.exp(action_log_probs - old_log_probs_t[batch_idx])
                ratio = torch.clamp(ratio, 0.0, 10.0)
                
                surr1 = ratio * advantages_t[batch_idx]
                surr2 = torch.clamp(ratio, 1 - CONFIG['clip_range'], 1 + CONFIG['clip_range']) * advantages_t[batch_idx]
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # Value loss
                value_loss = F.mse_loss(values_pred, returns_t[batch_idx])
                
                # Entropy (FP16-safe clamp values)
                probs = torch.exp(log_probs)
                valid_probs = probs.clamp(min=1e-10)
                valid_log_probs = log_probs.clamp(min=-1e4)  # FP16-safe
                entropy = -(valid_probs * valid_log_probs).sum(dim=-1).mean()
                
                # Total loss
                loss = (
                    policy_loss +
                    CONFIG['value_coef'] * value_loss -
                    CONFIG['entropy_coef'] * entropy
                )
            
            # KL divergence (outside autocast for accuracy)
            with torch.no_grad():
                kl = (old_log_probs_t[batch_idx] - action_log_probs.float()).mean()
            
            # Check for NaN in loss
            if check_for_nan(loss, "loss"):
                continue
            
            # Backward pass with scaler
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            
            # Unscale before clipping
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(network.parameters(), CONFIG['max_grad_norm'])
            
            # Optimizer step with scaler
            scaler.step(optimizer)
            scaler.update()
            
            # Record stats
            all_policy_loss.append(policy_loss.item())
            all_value_loss.append(value_loss.item())
            all_entropy.append(entropy.item())
            all_kl.append(kl.item())
    
    return {
        'policy_loss': np.mean(all_policy_loss) if all_policy_loss else 0,
        'value_loss': np.mean(all_value_loss) if all_value_loss else 0,
        'entropy': np.mean(all_entropy) if all_entropy else 0,
        'kl': np.mean(all_kl) if all_kl else 0,
    }

print("‚úÖ PPO update function defined!")

# ==============================================================================
# Cell 9: Evaluation Functions
# ==============================================================================

def evaluate_vs_random(n_games=20):
    """Evaluasi melawan random player."""
    network.eval()
    wins = 0
    draws = 0
    
    for game in range(n_games):
        state, _ = env.reset()
        done = False
        
        while not done:
            legal_mask = env.get_legal_action_mask()
            
            if legal_mask.sum() == 0:
                break
            
            if env.board.turn:  # AI plays white
                action, _, _ = select_action(state, legal_mask)
            else:  # Random plays black
                legal_actions = np.where(legal_mask)[0]
                action = np.random.choice(legal_actions)
            
            state, _, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
        
        result = env.board.result()
        if result == '1-0':
            wins += 1
        elif result == '1/2-1/2':
            draws += 1
    
    network.train()
    return wins / n_games, draws / n_games

def save_checkpoint(filename, update, history, best_win_rate):
    """Save training checkpoint (handles DataParallel)."""
    checkpoint = {
        'update': update,
        'network_state_dict': get_model().state_dict(),  # Unwrap DataParallel
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'scaler_state_dict': scaler.state_dict(),  # Save scaler state too
        'history': history,
        'config': CONFIG,
        'best_win_rate': best_win_rate,
    }
    torch.save(checkpoint, filename)
    return filename

def load_checkpoint(filename):
    """Load training checkpoint (handles DataParallel)."""
    checkpoint = torch.load(filename, map_location=device)
    get_model().load_state_dict(checkpoint['network_state_dict'])  # Load to unwrapped model
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    if 'scaler_state_dict' in checkpoint:
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
    return checkpoint.get('update', 0), checkpoint.get('history', {}), checkpoint.get('best_win_rate', 0)

print("‚úÖ Evaluation functions defined!")

# ==============================================================================
# Cell 10: Main Training Loop (V3 - Curriculum Learning)
# ==============================================================================

# Training history
history = {
    'policy_loss': [], 'value_loss': [], 'entropy': [], 'kl': [],
    'rewards': [], 'game_lengths': [], 'lr': [], 'win_rate': [], 'phase': []
}

# Best model tracking
best_win_rate = 0.0

# Checkpoint paths (Kaggle)
CHECKPOINT_DIR = '/kaggle/working'
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
print(f"‚úÖ Checkpoint directory: {CHECKPOINT_DIR}")

# Steps per env
STEPS_PER_ENV = CONFIG['n_steps'] // NUM_ENVS

# Calculate total updates across all phases
total_curriculum_updates = sum(p['updates'] for p in CONFIG['curriculum_phases'])

print("=" * 60)
print("üéì CURRICULUM LEARNING TRAINING")
print("=" * 60)
print(f"   Total phases: {len(CONFIG['curriculum_phases'])}")
for i, phase in enumerate(CONFIG['curriculum_phases']):
    print(f"   Phase {i+1}: {phase['name']} ({phase['updates']} updates)")
print(f"   Total updates: {total_curriculum_updates}")
print(f"   Parallel envs: {NUM_ENVS}")
print(f"   Batch size: {CONFIG['batch_size']}")
print("=" * 60)

start_time = time.time()
global_update = 0

# Initialize all environments
env_states = []
for i, e in enumerate(envs):
    s, _ = e.reset()
    env_states.append(s)

# CURRICULUM LEARNING LOOP
for phase_idx, phase in enumerate(CONFIG['curriculum_phases']):
    phase_name = phase['name']
    phase_opponent = phase['opponent']
    phase_updates = phase['updates']
    phase_target = phase['target_winrate']
    
    print(f"\n{'='*60}")
    print(f"üéØ {phase_name}")
    print(f"   Opponent: {phase_opponent}")
    print(f"   Updates: {phase_updates}")
    print(f"   Target win rate: {phase_target:.0%}")
    print(f"{'='*60}")
    
    # Update all environments with new opponent
    for e in envs:
        e.set_opponent(phase_opponent)
    
    phase_start = time.time()
    
    for update in tqdm(range(phase_updates), desc=f"Phase {phase_idx+1}"):
        # Collect rollout from ALL environments in parallel
        states = []
        actions = []
        rewards = []
        dones = []
        old_log_probs = []
        values = []
        masks = []
        
        episode_rewards = []
        episode_lengths = []
        env_episode_rewards = [0.0] * NUM_ENVS
        env_episode_lengths = [0] * NUM_ENVS
        
        # Each env does STEPS_PER_ENV steps - using BATCHED inference
        for step in range(STEPS_PER_ENV):
            # Get legal masks for all envs
            current_states = []
            current_masks = []
            
            for env_idx, (e, state) in enumerate(zip(envs, env_states)):
                legal_mask = e.get_legal_action_mask()
                
                # Reset if game ended
                if legal_mask.sum() == 0:
                    state, _ = e.reset()
                    env_states[env_idx] = state
                    legal_mask = e.get_legal_action_mask()
                
                current_states.append(env_states[env_idx])
                current_masks.append(legal_mask)
            
            # BATCH INFERENCE: ONE forward pass for ALL environments!
            batch_actions, batch_log_probs, batch_values = select_actions_batch(current_states, current_masks)
            
            # Apply actions to each environment
            for env_idx, e in enumerate(envs):
                action = batch_actions[env_idx]
                log_prob = batch_log_probs[env_idx]
                value = batch_values[env_idx]
                
                next_state, reward, terminated, truncated, _ = e.step(action)
                done = terminated or truncated
                
                states.append(current_states[env_idx])
                actions.append(action)
                rewards.append(reward)
                dones.append(float(done))
                old_log_probs.append(log_prob)
                values.append(value)
                masks.append(current_masks[env_idx])
                
                env_episode_rewards[env_idx] += reward
                env_episode_lengths[env_idx] += 1
                
                if done:
                    episode_rewards.append(env_episode_rewards[env_idx])
                    episode_lengths.append(env_episode_lengths[env_idx])
                    env_episode_rewards[env_idx] = 0.0
                    env_episode_lengths[env_idx] = 0
                    next_state, _ = e.reset()
                
                env_states[env_idx] = next_state
        
        # Compute last values for GAE (one per env)
        last_values = []
        with torch.no_grad():
            for state in env_states:
                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
                _, last_value = network(state_t)
                last_values.append(last_value.item())
        
        # Use mean last value for GAE
        avg_last_value = np.mean(last_values)
        
        # Compute GAE
        advantages, returns = compute_gae(rewards, values, dones, avg_last_value)
        
        # PPO update
        stats = ppo_update(states, actions, old_log_probs, advantages, returns, masks)
        
        # Update scheduler
        scheduler.step()
        
        # Record history
        history['policy_loss'].append(stats['policy_loss'])
        history['value_loss'].append(stats['value_loss'])
        history['entropy'].append(stats['entropy'])
        history['kl'].append(stats['kl'])
        history['rewards'].append(np.mean(episode_rewards) if episode_rewards else 0)
        history['game_lengths'].append(np.mean(episode_lengths) if episode_lengths else 0)
        history['lr'].append(scheduler.get_last_lr()[0])
        
        # Logging (only at intervals)
        if (update + 1) % CONFIG['log_interval'] == 0:
            elapsed = time.time() - start_time
            print(f"\n[Phase {phase_idx+1}] Update {update+1}/{phase_updates} | Time: {elapsed/60:.1f}min")
            print(f"  PolicyL: {stats['policy_loss']:.4f} | ValueL: {stats['value_loss']:.4f}")
            print(f"  Entropy: {stats['entropy']:.4f} | KL: {stats['kl']:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}")
            if episode_rewards:
                print(f"  Avg Reward: {np.mean(episode_rewards):.4f} | Avg Length: {np.mean(episode_lengths):.1f}")
            
            # Stability warnings
            if stats['entropy'] < CONFIG['min_entropy']:
                print(f"  ‚ö†Ô∏è Low entropy: {stats['entropy']:.4f}")
            if stats['kl'] > CONFIG['max_kl']:
                print(f"  ‚ö†Ô∏è High KL divergence: {stats['kl']:.4f}")
        
        # Evaluation
        if (update + 1) % CONFIG['eval_interval'] == 0:
            win_rate, draw_rate = evaluate_vs_random(20)
            history['win_rate'].append(win_rate)
            history['phase'].append(phase_idx + 1)
            print(f"  üìä vs Random: Win={win_rate:.1%}, Draw={draw_rate:.1%}")
            
            if win_rate > best_win_rate:
                best_win_rate = win_rate
                save_checkpoint(f'{CHECKPOINT_DIR}/chess_model_best.pt', global_update, history, best_win_rate)
                print(f"  üíæ New best model saved! (win_rate={best_win_rate:.1%})")
        
        # Checkpointing
        if (update + 1) % CONFIG['save_interval'] == 0:
            save_checkpoint(f'{CHECKPOINT_DIR}/chess_model_checkpoint.pt', global_update, history, best_win_rate)
            print(f"  üíæ Checkpoint saved at update {global_update+1}")
        
        global_update += 1
    
    # Phase complete - save and report
    phase_time = time.time() - phase_start
    print(f"\n‚úÖ {phase_name} COMPLETE!")
    print(f"   Time: {phase_time/60:.1f} min")
    print(f"   Best win rate: {best_win_rate:.1%}")
    save_checkpoint(f'{CHECKPOINT_DIR}/chess_model_phase{phase_idx+1}.pt', global_update, history, best_win_rate)

# Final save
save_checkpoint(f'{CHECKPOINT_DIR}/chess_model_final.pt', global_update, history, best_win_rate)
print("\n" + "=" * 60)
print("üèÅ CURRICULUM LEARNING COMPLETE!")
print("=" * 60)
print(f"   Total time: {(time.time() - start_time)/60:.1f} minutes")
print(f"   Total updates: {global_update}")
print(f"   Best win rate: {best_win_rate:.1%}")

# ==============================================================================
# Cell 11: Enhanced Visualization & Results
# ==============================================================================

def smooth_curve(values, weight=0.9):
    """Smooth curve dengan exponential moving average."""
    smoothed = []
    last = values[0] if values else 0
    for point in values:
        smoothed_val = last * weight + (1 - weight) * point
        smoothed.append(smoothed_val)
        last = smoothed_val
    return smoothed

def plot_training_curves():
    """Plot comprehensive training curves."""
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    
    # Policy Loss
    axes[0, 0].plot(history['policy_loss'], alpha=0.3, color='blue')
    axes[0, 0].plot(smooth_curve(history['policy_loss']), color='blue', linewidth=2)
    axes[0, 0].set_title('Policy Loss')
    axes[0, 0].set_xlabel('Update')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Value Loss
    axes[0, 1].plot(history['value_loss'], alpha=0.3, color='orange')
    axes[0, 1].plot(smooth_curve(history['value_loss']), color='orange', linewidth=2)
    axes[0, 1].set_title('Value Loss')
    axes[0, 1].set_xlabel('Update')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Entropy
    axes[0, 2].plot(history['entropy'], alpha=0.3, color='green')
    axes[0, 2].plot(smooth_curve(history['entropy']), color='green', linewidth=2)
    axes[0, 2].axhline(y=CONFIG['min_entropy'], color='r', linestyle='--', label='Min Threshold')
    axes[0, 2].set_title('Entropy (Exploration)')
    axes[0, 2].set_xlabel('Update')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # KL Divergence
    axes[0, 3].plot(history['kl'], alpha=0.3, color='purple')
    axes[0, 3].plot(smooth_curve(history['kl']), color='purple', linewidth=2)
    axes[0, 3].axhline(y=CONFIG['max_kl'], color='r', linestyle='--', label='Max Threshold')
    axes[0, 3].set_title('KL Divergence')
    axes[0, 3].set_xlabel('Update')
    axes[0, 3].legend()
    axes[0, 3].grid(True, alpha=0.3)
    
    # Rewards
    axes[1, 0].plot(history['rewards'], alpha=0.3, color='red')
    axes[1, 0].plot(smooth_curve(history['rewards']), color='red', linewidth=2)
    axes[1, 0].set_title('Average Episode Reward')
    axes[1, 0].set_xlabel('Update')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Game Lengths
    axes[1, 1].plot(history['game_lengths'], alpha=0.3, color='cyan')
    axes[1, 1].plot(smooth_curve(history['game_lengths']), color='cyan', linewidth=2)
    axes[1, 1].set_title('Average Game Length')
    axes[1, 1].set_xlabel('Update')
    axes[1, 1].grid(True, alpha=0.3)
    
    # Win Rate
    if history['win_rate']:
        x_vals = [(i + 1) * CONFIG['eval_interval'] for i in range(len(history['win_rate']))]
        axes[1, 2].plot(x_vals, history['win_rate'], 'go-', markersize=6, linewidth=2)
        axes[1, 2].fill_between(x_vals, 0, history['win_rate'], alpha=0.3, color='green')
        axes[1, 2].set_title('Win Rate vs Random')
        axes[1, 2].set_xlabel('Update')
        axes[1, 2].set_ylim(0, 1)
        axes[1, 2].grid(True, alpha=0.3)
    
    # Learning Rate
    axes[1, 3].plot(history['lr'], color='brown', linewidth=2)
    axes[1, 3].set_title('Learning Rate Schedule')
    axes[1, 3].set_xlabel('Update')
    axes[1, 3].set_yscale('log')
    axes[1, 3].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{CHECKPOINT_DIR}/training_curves.png', dpi=150, bbox_inches='tight')
    plt.show()
    print(f"üìä Training curves saved to {CHECKPOINT_DIR}/training_curves.png")

# Plot curves
plot_training_curves()

# Final evaluation
print("\nüìä Final Evaluation (50 games):")
final_win_rate, final_draw_rate = evaluate_vs_random(50)
print(f"   Win Rate: {final_win_rate:.1%}")
print(f"   Draw Rate: {final_draw_rate:.1%}")
print(f"   Loss Rate: {1 - final_win_rate - final_draw_rate:.1%}")

print("\n‚úÖ Training complete! Model saved to:")
print(f"   Best: {CHECKPOINT_DIR}/chess_model_best.pt")
print(f"   Final: {CHECKPOINT_DIR}/chess_model_final.pt")

# ==============================================================================
# Cell 12: Opponents (Self-Play, Random, Stockfish)
# ==============================================================================

class RandomOpponent:
    """Random player - pilih move secara acak."""
    
    def __init__(self):
        self.name = "Random"
    
    def select_action(self, env):
        """Select random legal action."""
        legal_mask = env.get_legal_action_mask()
        legal_actions = np.where(legal_mask)[0]
        if len(legal_actions) == 0:
            return None
        return np.random.choice(legal_actions)

class SelfPlayOpponent:
    """Self-play opponent - gunakan network dengan snapshot weights."""
    
    def __init__(self, network, device):
        self.name = "SelfPlay"
        self.network = network
        self.device = device
        # Copy weights awal
        self.snapshot_weights = None
        self.update_snapshot()
    
    def update_snapshot(self):
        """Update snapshot weights dari current network."""
        self.snapshot_weights = {k: v.clone() for k, v in self.network.state_dict().items()}
    
    def select_action(self, env):
        """Select action menggunakan snapshot weights."""
        # Temporarily load snapshot weights
        current_weights = {k: v.clone() for k, v in self.network.state_dict().items()}
        self.network.load_state_dict(self.snapshot_weights)
        
        state = env.encode_state()
        legal_mask = env.get_legal_action_mask()
        
        if legal_mask.sum() == 0:
            self.network.load_state_dict(current_weights)
            return None
        
        self.network.eval()
        with torch.no_grad():
            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            mask_t = torch.BoolTensor(legal_mask).unsqueeze(0).to(self.device)
            log_probs, _ = self.network(state_t, mask_t)
            dist = Categorical(logits=log_probs)
            action = dist.sample().item()
        
        # Restore current weights
        self.network.load_state_dict(current_weights)
        self.network.train()
        
        return action

class StockfishOpponent:
    """Stockfish opponent - berbagai level skill."""
    
    def __init__(self, level=0, time_limit_ms=100):
        self.name = f"Stockfish_L{level}"
        self.level = level
        self.time_limit_ms = time_limit_ms
        self.engine = None
        self._init_engine()
    
    def _init_engine(self):
        """Initialize Stockfish engine."""
        try:
            from stockfish import Stockfish
            # Path untuk Colab
            paths = ['/usr/games/stockfish', '/usr/bin/stockfish', 'stockfish']
            for path in paths:
                try:
                    self.engine = Stockfish(path=path)
                    self.engine.set_skill_level(self.level)
                    print(f"‚úÖ Stockfish initialized at level {self.level}")
                    break
                except:
                    continue
            if self.engine is None:
                print("‚ö†Ô∏è Stockfish not found, falling back to random")
        except ImportError:
            print("‚ö†Ô∏è stockfish package not installed")
    
    def select_action(self, env):
        """Select action menggunakan Stockfish."""
        if self.engine is None:
            # Fallback to random
            legal_mask = env.get_legal_action_mask()
            legal_actions = np.where(legal_mask)[0]
            return np.random.choice(legal_actions) if len(legal_actions) > 0 else None
        
        try:
            self.engine.set_fen_position(env.board.fen())
            best_move = self.engine.get_best_move_time(self.time_limit_ms)
            if best_move:
                move = chess.Move.from_uci(best_move)
                uci = move.uci()
                if uci in env.move_to_action:
                    return env.move_to_action[uci]
                # Handle promotions
                if len(uci) == 5:
                    base = uci[:4]
                    if base in env.move_to_action:
                        return env.move_to_action[base]
        except Exception as e:
            print(f"‚ö†Ô∏è Stockfish error: {e}")
        
        # Fallback
        legal_mask = env.get_legal_action_mask()
        legal_actions = np.where(legal_mask)[0]
        return np.random.choice(legal_actions) if len(legal_actions) > 0 else None

print("‚úÖ Opponent classes defined!")

# ==============================================================================
# Cell 13: Evaluation vs Different Opponents
# ==============================================================================

def evaluate_vs_opponent(opponent, n_games=20, ai_plays_white=True, verbose=False):
    """Evaluasi melawan opponent tertentu."""
    network.eval()
    wins = 0
    draws = 0
    losses = 0
    total_moves = 0
    
    for game in range(n_games):
        state, _ = env.reset()
        done = False
        moves = 0
        
        while not done:
            legal_mask = env.get_legal_action_mask()
            
            if legal_mask.sum() == 0:
                break
            
            is_ai_turn = (env.board.turn == chess.WHITE) == ai_plays_white
            
            if is_ai_turn:
                action, _, _ = select_action(state, legal_mask)
            else:
                action = opponent.select_action(env)
                if action is None:
                    break
            
            state, _, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            moves += 1
        
        total_moves += moves
        result = env.board.result()
        
        if ai_plays_white:
            if result == '1-0':
                wins += 1
            elif result == '0-1':
                losses += 1
            else:
                draws += 1
        else:
            if result == '0-1':
                wins += 1
            elif result == '1-0':
                losses += 1
            else:
                draws += 1
        
        if verbose and (game + 1) % 5 == 0:
            print(f"   Game {game+1}/{n_games} complete")
    
    network.train()
    
    return {
        'wins': wins,
        'draws': draws,
        'losses': losses,
        'win_rate': wins / n_games,
        'draw_rate': draws / n_games,
        'avg_moves': total_moves / n_games
    }

def comprehensive_evaluation():
    """Evaluasi lengkap melawan berbagai opponents."""
    print("\n" + "=" * 60)
    print("üìä COMPREHENSIVE EVALUATION")
    print("=" * 60)
    
    results = {}
    
    # vs Random
    print("\nüé≤ Evaluating vs Random (50 games)...")
    random_opp = RandomOpponent()
    results['random'] = evaluate_vs_opponent(random_opp, n_games=50, verbose=True)
    print(f"   Win: {results['random']['win_rate']:.1%} | Draw: {results['random']['draw_rate']:.1%}")
    
    # vs Self-Play (if applicable)
    print("\nüîÑ Evaluating vs Self (20 games)...")
    self_opp = SelfPlayOpponent(network, device)
    results['self'] = evaluate_vs_opponent(self_opp, n_games=20, verbose=True)
    print(f"   Win: {results['self']['win_rate']:.1%} | Draw: {results['self']['draw_rate']:.1%}")
    
    # vs Stockfish (if available)
    try:
        for level in [0, 1, 2]:
            print(f"\n‚ôüÔ∏è Evaluating vs Stockfish Level {level} (10 games)...")
            sf_opp = StockfishOpponent(level=level)
            if sf_opp.engine is not None:
                results[f'stockfish_l{level}'] = evaluate_vs_opponent(sf_opp, n_games=10, verbose=True)
                print(f"   Win: {results[f'stockfish_l{level}']['win_rate']:.1%} | Draw: {results[f'stockfish_l{level}']['draw_rate']:.1%}")
    except Exception as e:
        print(f"   Stockfish evaluation skipped: {e}")
    
    print("\n" + "=" * 60)
    print("üìã EVALUATION SUMMARY")
    print("=" * 60)
    for name, res in results.items():
        print(f"   {name:15s}: W={res['win_rate']:.1%} D={res['draw_rate']:.1%} L={1-res['win_rate']-res['draw_rate']:.1%}")
    
    return results

# Run comprehensive evaluation
evaluation_results = comprehensive_evaluation()

# ==============================================================================
# Cell 14: Resume Training Capability
# ==============================================================================

def resume_training(checkpoint_path, additional_updates=1000):
    """Resume training dari checkpoint."""
    global history, best_win_rate
    
    print(f"üìÇ Loading checkpoint from {checkpoint_path}...")
    
    try:
        start_update, loaded_history, best_win_rate = load_checkpoint(checkpoint_path)
        history = loaded_history if loaded_history else history
        print(f"‚úÖ Resumed from update {start_update}")
        print(f"   Best win rate: {best_win_rate:.1%}")
    except Exception as e:
        print(f"‚ùå Failed to load checkpoint: {e}")
        return
    
    # Continue training
    end_update = start_update + additional_updates
    print(f"\nüöÄ Continuing training for {additional_updates} more updates...")
    print(f"   From update {start_update} to {end_update}")
    
    start_time = time.time()
    
    for update in tqdm(range(start_update, end_update), desc="Resuming"):
        # (Same training loop as Cell 10)
        states, actions, rewards, dones = [], [], [], []
        old_log_probs, values, masks = [], [], []
        
        state, _ = env.reset()
        episode_reward = 0
        episode_rewards, episode_lengths = [], []
        episode_length = 0
        
        for step in range(CONFIG['n_steps']):
            legal_mask = env.get_legal_action_mask()
            if legal_mask.sum() == 0:
                state, _ = env.reset()
                legal_mask = env.get_legal_action_mask()
            
            action, log_prob, value = select_action(state, legal_mask)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            dones.append(float(done))
            old_log_probs.append(log_prob)
            values.append(value)
            masks.append(legal_mask)
            
            episode_reward += reward
            episode_length += 1
            
            if done:
                episode_rewards.append(episode_reward)
                episode_lengths.append(episode_length)
                episode_reward = 0
                episode_length = 0
                state, _ = env.reset()
            else:
                state = next_state
        
        with torch.no_grad():
            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
            _, last_value = network(state_t)
            last_value = last_value.item()
        
        advantages, returns = compute_gae(rewards, values, dones, last_value)
        stats = ppo_update(states, actions, old_log_probs, advantages, returns, masks)
        scheduler.step()
        
        # Record history
        history['policy_loss'].append(stats['policy_loss'])
        history['value_loss'].append(stats['value_loss'])
        history['entropy'].append(stats['entropy'])
        history['kl'].append(stats['kl'])
        history['rewards'].append(np.mean(episode_rewards) if episode_rewards else 0)
        history['game_lengths'].append(np.mean(episode_lengths) if episode_lengths else 0)
        history['lr'].append(scheduler.get_last_lr()[0])
        
        if (update + 1) % CONFIG['log_interval'] == 0:
            elapsed = time.time() - start_time
            print(f"\nUpdate {update+1} | Time: {elapsed/60:.1f}min")
            print(f"  PolicyL: {stats['policy_loss']:.4f} | Entropy: {stats['entropy']:.4f}")
        
        if (update + 1) % CONFIG['eval_interval'] == 0:
            win_rate, draw_rate = evaluate_vs_random(20)
            history['win_rate'].append(win_rate)
            print(f"  üìä vs Random: Win={win_rate:.1%}")
            
            if win_rate > best_win_rate:
                best_win_rate = win_rate
                save_checkpoint(f'{CHECKPOINT_DIR}/chess_model_best.pt', update, history, best_win_rate)
                print(f"  üíæ New best!")
        
        if (update + 1) % CONFIG['save_interval'] == 0:
            save_checkpoint(f'{CHECKPOINT_DIR}/chess_model_checkpoint.pt', update, history, best_win_rate)
    
    save_checkpoint(f'{CHECKPOINT_DIR}/chess_model_final.pt', end_update, history, best_win_rate)
    print(f"\n‚úÖ Resume training complete! Total updates: {end_update}")

# Uncomment to resume training:
# resume_training('/content/chess_model_checkpoint.pt', additional_updates=2000)

print("‚úÖ Resume training function defined!")

# ==============================================================================
# Cell 15: Play Demo Game
# ==============================================================================

def display_board_text(board):
    """Display board sebagai text (untuk Colab)."""
    print(board)
    print()

def play_demo_game(opponent_type='random', verbose=True, max_moves=100):
    """Main sebuah demo game dan tampilkan moves."""
    print("\n" + "=" * 60)
    print("üéÆ DEMO GAME")
    print("=" * 60)
    
    if opponent_type == 'random':
        opponent = RandomOpponent()
    elif opponent_type == 'self':
        opponent = SelfPlayOpponent(network, device)
    else:
        opponent = RandomOpponent()
    
    print(f"AI (White) vs {opponent.name} (Black)")
    print("=" * 60)
    
    state, _ = env.reset()
    move_history = []
    
    if verbose:
        print("\nInitial Position:")
        display_board_text(env.board)
    
    network.eval()
    
    for move_num in range(max_moves):
        legal_mask = env.get_legal_action_mask()
        
        if legal_mask.sum() == 0:
            break
        
        if env.board.turn == chess.WHITE:
            # AI move
            action, _, _ = select_action(state, legal_mask)
            move = env.action_to_move.get(action, None)
            
            # Find actual legal move
            for legal in env.board.legal_moves:
                if move and legal.uci()[:4] == move.uci()[:4]:
                    move = legal
                    break
            
            player = "AI"
        else:
            # Opponent move
            action = opponent.select_action(env)
            move = env.action_to_move.get(action, None)
            for legal in env.board.legal_moves:
                if move and legal.uci()[:4] == move.uci()[:4]:
                    move = legal
                    break
            player = opponent.name
        
        if move:
            san = env.board.san(move)
            move_history.append(san)
            
            if verbose:
                move_idx = len(move_history)
                if env.board.turn == chess.WHITE:
                    print(f"{(move_idx + 1) // 2}. {san}", end=" ")
                else:
                    print(san)
        
        state, reward, terminated, truncated, _ = env.step(action)
        
        if terminated or truncated:
            break
    
    network.train()
    
    print("\n")
    print("=" * 60)
    print("FINAL POSITION:")
    display_board_text(env.board)
    
    result = env.board.result()
    print(f"Result: {result}")
    
    if result == '1-0':
        print("üèÜ AI WINS!")
    elif result == '0-1':
        print("üò¢ AI LOSES")
    else:
        print("ü§ù DRAW")
    
    # Print move history in PGN format
    print("\nüìú Move History (PGN):")
    pgn_str = ""
    for i, move in enumerate(move_history):
        if i % 2 == 0:
            pgn_str += f"{i // 2 + 1}. "
        pgn_str += move + " "
    print(pgn_str)
    
    return result, move_history

# Play a demo game
demo_result, demo_moves = play_demo_game(opponent_type='random', verbose=True)

# ==============================================================================
# Cell 16: Export Model for Production
# ==============================================================================

def export_model(path=None, include_optimizer=False):
    """Export model untuk production use."""
    if path is None:
        path = f'{CHECKPOINT_DIR}/chess_model_export.pt'
    
    export_dict = {
        'network_state_dict': network.state_dict(),
        'config': CONFIG,
        'action_to_move': {k: v.uci() for k, v in env.action_to_move.items()},
        'move_to_action': env.move_to_action,
        'best_win_rate': best_win_rate,
    }
    
    if include_optimizer:
        export_dict['optimizer_state_dict'] = optimizer.state_dict()
        export_dict['scheduler_state_dict'] = scheduler.state_dict()
    
    torch.save(export_dict, path)
    print(f"‚úÖ Model exported to: {path}")
    
    # Print model info
    print(f"\nüìã Model Info:")
    print(f"   Parameters: {sum(p.numel() for p in network.parameters()):,}")
    print(f"   Best Win Rate: {best_win_rate:.1%}")
    print(f"   Network Config: {CONFIG['num_filters']} filters, {CONFIG['num_blocks']} blocks")
    
    return path

# Export the model
exported_path = export_model()

print("\n" + "=" * 60)
print("üéâ ALL DONE!")
print("=" * 60)
print("Files saved:")
print(f"   üìÅ {CHECKPOINT_DIR}/chess_model_best.pt")
print(f"   üìÅ {CHECKPOINT_DIR}/chess_model_final.pt")
print(f"   üìÅ {CHECKPOINT_DIR}/chess_model_export.pt")
print(f"   üìä {CHECKPOINT_DIR}/training_curves.png")
print("\nTo continue training later:")
print("   resume_training('/content/chess_model_checkpoint.pt', additional_updates=2000)")
print("\nTo play a demo game:")
print("   play_demo_game(opponent_type='random')")
print("=" * 60)
