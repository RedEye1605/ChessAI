"""
================================================================================
üèÜ ALPHAZERO-STYLE CHESS TRAINING SCRIPT
================================================================================
Script lengkap untuk training AI Catur dengan MCTS + Neural Network.
Terinspirasi dari AlphaZero dan Leela Chess Zero (LCZero).

Fitur Utama:
- Monte Carlo Tree Search (MCTS) dengan PUCT formula
- Self-play untuk generasi data training
- Policy-Value Network dengan SE blocks
- Temperature-based move selection
- Training buffer untuk experience replay
- WDL value head option

Dapat dipindahkan ke Kaggle Notebook dengan memisahkan berdasarkan "# Cell X"

Author: Generated by AI Assistant
Date: 2026-01-15
================================================================================
"""

# ==============================================================================
# Cell 1: Setup & GPU Check (Optimized for Kaggle P100)
# ==============================================================================

import torch
import numpy as np
import random
import os
import sys
import time
import math
import warnings
import copy
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
from collections import deque
import threading
from concurrent.futures import ThreadPoolExecutor

warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

set_seed(42)

# GPU setup
N_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0
USE_FP16 = torch.cuda.is_available()

if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"‚úÖ Found {N_GPUS} GPU(s):")
    total_mem = 0
    for i in range(N_GPUS):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_mem = torch.cuda.get_device_properties(i).total_memory / 1e9
        total_mem += gpu_mem
        print(f"   GPU {i}: {gpu_name} ({gpu_mem:.1f} GB)")
    print(f"   Total VRAM: {total_mem:.1f} GB")
    if USE_FP16:
        print(f"‚úÖ Mixed Precision (FP16) enabled")
else:
    device = torch.device('cpu')
    print("‚ö†Ô∏è GPU tidak tersedia! Training akan sangat lambat.")

print(f"‚úÖ Device: {device}")
print(f"‚úÖ PyTorch version: {torch.__version__}")

# ==============================================================================
# Cell 2: Install Dependencies
# ==============================================================================

# Uncomment these lines when running in Kaggle:
# !pip install -q python-chess gymnasium tqdm matplotlib
# print("‚úÖ Dependencies installed!")

try:
    import chess
    import gymnasium as gym
    from gymnasium import spaces
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    print("‚úÖ All dependencies loaded!")
except ImportError as e:
    print(f"‚ùå Missing dependency: {e}")
    print("   Run: pip install python-chess gymnasium tqdm matplotlib")

# ==============================================================================
# Cell 3: Configuration (AlphaZero-Style)
# ==============================================================================

@dataclass
class AlphaZeroConfig:
    """Configuration untuk AlphaZero-style training."""
    
    # === Network Architecture ===
    input_channels: int = 18           # State encoding channels
    num_filters: int = 256             # Filters per layer
    num_blocks: int = 12               # Residual blocks (12 for P100, 20 for V100+)
    action_size: int = 4672            # AlphaZero action space
    use_se: bool = True                # Squeeze-and-Excitation blocks
    use_wdl: bool = True               # Win-Draw-Loss value head
    
    # === MCTS Parameters ===
    num_simulations: int = 100         # MCTS simulations per move (100 for fast, 800 for strong)
    c_puct: float = 1.5                # Exploration constant
    dirichlet_alpha: float = 0.3       # Dirichlet noise alpha for root
    dirichlet_epsilon: float = 0.25    # Fraction of noise to add
    temperature_init: float = 1.0      # Initial temperature
    temperature_final: float = 0.1     # Final temperature
    temperature_drop_move: int = 30    # Move to switch to final temperature
    
    # === Self-Play ===
    games_per_iteration: int = 50      # Self-play games per training iteration
    max_game_length: int = 300         # Maximum moves per game
    resign_threshold: float = -0.95    # Resign if value below this
    
    # === Training ===
    batch_size: int = 512              # Training batch size
    learning_rate: float = 2e-4        # Initial learning rate
    weight_decay: float = 1e-4         # L2 regularization
    lr_warmup_steps: int = 1000        # LR warmup steps
    training_steps_per_iter: int = 500 # Training steps per iteration
    
    # === Buffer ===
    buffer_size: int = 200000          # Replay buffer size (positions)
    min_buffer_size: int = 5000        # Minimum buffer before training
    
    # === Training Loop ===
    num_iterations: int = 1000         # Total training iterations
    eval_interval: int = 10            # Evaluate every N iterations
    save_interval: int = 50            # Save checkpoint every N iterations
    log_interval: int = 5              # Log every N iterations
    
    # === Evaluation ===
    eval_games: int = 20               # Games for evaluation
    arena_games: int = 40              # Games for model comparison
    arena_threshold: float = 0.55      # Win rate needed to update best model
    
    # === Hardware ===
    use_fp16: bool = USE_FP16          # Mixed precision
    num_workers: int = 4               # Parallel self-play workers

CONFIG = AlphaZeroConfig()

print("üìã AlphaZero Configuration:")
for field_name, field_value in CONFIG.__dict__.items():
    print(f"   {field_name}: {field_value}")

# ==============================================================================
# Cell 4: State Encoder (Enhanced)
# ==============================================================================

class StateEncoder:
    """
    Encode chess board state untuk neural network.
    
    Encoding (18 channels):
    - 12 piece planes (6 white + 6 black)
    - 1 turn indicator
    - 4 castling rights
    - 1 en passant
    """
    
    PIECE_ORDER = [chess.PAWN, chess.KNIGHT, chess.BISHOP, 
                   chess.ROOK, chess.QUEEN, chess.KING]
    
    def __init__(self):
        self.num_channels = 18
    
    def encode(self, board: chess.Board) -> np.ndarray:
        """Encode single board state."""
        state = np.zeros((18, 8, 8), dtype=np.float32)
        
        # Piece planes
        for piece_type in self.PIECE_ORDER:
            # White pieces
            channel = piece_type - 1
            for sq in board.pieces(piece_type, chess.WHITE):
                rank, file = sq // 8, sq % 8
                state[channel, rank, file] = 1.0
            
            # Black pieces
            channel = piece_type - 1 + 6
            for sq in board.pieces(piece_type, chess.BLACK):
                rank, file = sq // 8, sq % 8
                state[channel, rank, file] = 1.0
        
        # Turn (channel 12)
        state[12, :, :] = 1.0 if board.turn == chess.WHITE else 0.0
        
        # Castling rights (channels 13-16)
        state[13, :, :] = float(board.has_kingside_castling_rights(chess.WHITE))
        state[14, :, :] = float(board.has_queenside_castling_rights(chess.WHITE))
        state[15, :, :] = float(board.has_kingside_castling_rights(chess.BLACK))
        state[16, :, :] = float(board.has_queenside_castling_rights(chess.BLACK))
        
        # En passant (channel 17)
        if board.ep_square is not None:
            ep_rank, ep_file = board.ep_square // 8, board.ep_square % 8
            state[17, ep_rank, ep_file] = 1.0
        
        return state
    
    def encode_batch(self, boards: List[chess.Board]) -> np.ndarray:
        """Encode batch of boards."""
        return np.array([self.encode(b) for b in boards])

print("‚úÖ StateEncoder defined!")

# ==============================================================================
# Cell 5: Action Space (AlphaZero-style)
# ==============================================================================

class ActionSpace:
    """
    AlphaZero-style action encoding.
    4672 possible moves covering all piece movements and promotions.
    """
    
    def __init__(self):
        self.action_to_move: Dict[int, chess.Move] = {}
        self.move_to_action: Dict[str, int] = {}
        self._init_encoding()
    
    def _init_encoding(self):
        """Initialize action space encoding."""
        # Direction vectors for queen-like moves
        directions = []
        for d in [(0, 1), (0, -1), (1, 0), (-1, 0), 
                  (1, 1), (1, -1), (-1, 1), (-1, -1)]:
            for dist in range(1, 8):
                directions.append((d[0] * dist, d[1] * dist))
        
        # Knight moves
        for d in [(1, 2), (2, 1), (2, -1), (1, -2), 
                  (-1, -2), (-2, -1), (-2, 1), (-1, 2)]:
            directions.append(d)
        
        action = 0
        for sq in range(64):
            from_rank, from_file = sq // 8, sq % 8
            
            for dx, dy in directions:
                to_rank = from_rank + dy
                to_file = from_file + dx
                
                if 0 <= to_rank < 8 and 0 <= to_file < 8:
                    to_sq = to_rank * 8 + to_file
                    move = chess.Move(sq, to_sq)
                    self.action_to_move[action] = move
                    self.move_to_action[move.uci()] = action
                action += 1
            
            # Underpromotions (pawn on 7th rank for white perspective)
            if from_rank == 6:
                for dx in [-1, 0, 1]:
                    for promo in [chess.KNIGHT, chess.BISHOP, chess.ROOK]:
                        to_file = from_file + dx
                        if 0 <= to_file < 8:
                            to_sq = 7 * 8 + to_file
                            move = chess.Move(sq, to_sq, promotion=promo)
                            self.action_to_move[action] = move
                            self.move_to_action[move.uci()] = action
                        action += 1
        
        self.num_actions = 4672
    
    def encode_move(self, move: chess.Move) -> int:
        """Encode move to action index."""
        uci = move.uci()
        if uci in self.move_to_action:
            return self.move_to_action[uci]
        # Handle queen promotions
        if len(uci) == 5 and uci[4] == 'q':
            base_uci = uci[:4]
            if base_uci in self.move_to_action:
                return self.move_to_action[base_uci]
        return -1
    
    def decode_action(self, action: int) -> Optional[chess.Move]:
        """Decode action index to move."""
        return self.action_to_move.get(action, None)
    
    def get_legal_action_mask(self, board: chess.Board) -> np.ndarray:
        """Get boolean mask for legal actions."""
        mask = np.zeros(self.num_actions, dtype=bool)
        for move in board.legal_moves:
            action = self.encode_move(move)
            if action >= 0:
                mask[action] = True
        return mask
    
    def get_legal_actions(self, board: chess.Board) -> List[int]:
        """Get list of legal action indices."""
        actions = []
        for move in board.legal_moves:
            action = self.encode_move(move)
            if action >= 0:
                actions.append(action)
        return actions

ACTION_SPACE = ActionSpace()
STATE_ENCODER = StateEncoder()

print(f"‚úÖ ActionSpace defined! ({ACTION_SPACE.num_actions} actions)")

# ==============================================================================
# Cell 6: Neural Network (Policy-Value with SE Blocks)
# ==============================================================================

import torch.nn as nn
import torch.nn.functional as F

class SEBlock(nn.Module):
    """Squeeze-and-Excitation block."""
    
    def __init__(self, channels: int, reduction: int = 4):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b, c, _, _ = x.size()
        y = self.pool(x).view(b, c)
        y = F.relu(self.fc1(y))
        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)
        return x * y

class ResidualBlock(nn.Module):
    """Residual block with optional SE attention."""
    
    def __init__(self, channels: int, use_se: bool = True):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(channels)
        self.se = SEBlock(channels) if use_se else nn.Identity()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.se(out)
        return F.relu(out + residual)

class AlphaZeroNetwork(nn.Module):
    """
    AlphaZero-style Policy-Value Network.
    
    Features:
    - Residual backbone with SE blocks
    - Dual head: Policy + Value (optional WDL)
    - Legal move masking
    """
    
    def __init__(
        self,
        input_channels: int = 18,
        num_filters: int = 256,
        num_blocks: int = 12,
        action_size: int = 4672,
        use_se: bool = True,
        use_wdl: bool = True
    ):
        super().__init__()
        
        self.action_size = action_size
        self.use_wdl = use_wdl
        
        # Input conv
        self.input_conv = nn.Sequential(
            nn.Conv2d(input_channels, num_filters, 3, padding=1, bias=False),
            nn.BatchNorm2d(num_filters),
            nn.ReLU(inplace=True)
        )
        
        # Residual tower
        self.res_blocks = nn.ModuleList([
            ResidualBlock(num_filters, use_se=(use_se and i % 2 == 0))
            for i in range(num_blocks)
        ])
        
        # Policy head
        self.policy_conv = nn.Conv2d(num_filters, 80, 1, bias=False)
        self.policy_bn = nn.BatchNorm2d(80)
        self.policy_fc = nn.Linear(80 * 64, action_size)
        
        # Value head
        self.value_conv = nn.Conv2d(num_filters, 32, 1, bias=False)
        self.value_bn = nn.BatchNorm2d(32)
        self.value_fc1 = nn.Linear(32 * 64, 256)
        
        if use_wdl:
            # WDL head: outputs Win, Draw, Loss probabilities
            self.value_fc2 = nn.Linear(256, 3)
        else:
            # Single value head
            self.value_fc2 = nn.Linear(256, 1)
        
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
        
        # Smaller init for output layers
        nn.init.xavier_uniform_(self.policy_fc.weight, gain=0.01)
        nn.init.zeros_(self.policy_fc.bias)
        nn.init.xavier_uniform_(self.value_fc2.weight, gain=0.01)
        nn.init.zeros_(self.value_fc2.bias)
    
    def forward(
        self, 
        x: torch.Tensor, 
        legal_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass.
        
        Args:
            x: Input tensor (batch, channels, 8, 8)
            legal_mask: Boolean mask for legal actions (batch, action_size)
            
        Returns:
            policy: Log probabilities (batch, action_size)
            value: Value estimate (batch, 1) or WDL (batch, 3)
        """
        # Backbone
        x = self.input_conv(x)
        for block in self.res_blocks:
            x = block(x)
        
        # Policy head
        policy = F.relu(self.policy_bn(self.policy_conv(x)))
        policy = policy.view(policy.size(0), -1)
        policy_logits = self.policy_fc(policy)
        
        # Mask illegal moves
        if legal_mask is not None:
            policy_logits = policy_logits.float()
            policy_logits = policy_logits.masked_fill(~legal_mask, -1e9)
        
        log_policy = F.log_softmax(policy_logits, dim=-1)
        
        # Value head
        value = F.relu(self.value_bn(self.value_conv(x)))
        value = value.view(value.size(0), -1)
        value = F.relu(self.value_fc1(value))
        value = self.value_fc2(value)
        
        if self.use_wdl:
            value = F.softmax(value, dim=-1)  # WDL probabilities
        else:
            value = torch.tanh(value)  # Single value in [-1, 1]
        
        return log_policy, value
    
    def get_policy_value(
        self, 
        state: np.ndarray, 
        legal_mask: np.ndarray
    ) -> Tuple[np.ndarray, float]:
        """Get policy and value for single state (for MCTS)."""
        self.eval()
        with torch.no_grad():
            state_t = torch.FloatTensor(state).unsqueeze(0).to(next(self.parameters()).device)
            mask_t = torch.BoolTensor(legal_mask).unsqueeze(0).to(next(self.parameters()).device)
            
            log_policy, value = self(state_t, mask_t)
            policy = torch.exp(log_policy).squeeze(0).cpu().numpy()
            
            if self.use_wdl:
                # Convert WDL to single value: W - L
                wdl = value.squeeze(0).cpu().numpy()
                v = wdl[0] - wdl[2]  # Win - Loss
            else:
                v = value.item()
        
        return policy, v

# Create network
network = AlphaZeroNetwork(
    input_channels=CONFIG.input_channels,
    num_filters=CONFIG.num_filters,
    num_blocks=CONFIG.num_blocks,
    action_size=CONFIG.action_size,
    use_se=CONFIG.use_se,
    use_wdl=CONFIG.use_wdl
).to(device)

num_params = sum(p.numel() for p in network.parameters())
print(f"‚úÖ AlphaZeroNetwork: {num_params:,} parameters")

# ==============================================================================
# Cell 7: MCTS Node & Tree
# ==============================================================================

class MCTSNode:
    """
    MCTS tree node dengan neural network priors.
    
    Stores:
    - visit_count: N(s,a) - berapa kali node dikunjungi
    - value_sum: W(s,a) - total value dari semua visits
    - prior: P(s,a) - prior probability dari neural network
    - children: child nodes
    """
    
    __slots__ = ['visit_count', 'value_sum', 'prior', 'children', 'to_play']
    
    def __init__(self, prior: float = 0.0):
        self.visit_count = 0
        self.value_sum = 0.0
        self.prior = prior
        self.children: Dict[int, 'MCTSNode'] = {}
        self.to_play = chess.WHITE
    
    def expanded(self) -> bool:
        """Check if node has been expanded."""
        return len(self.children) > 0
    
    def value(self) -> float:
        """Get mean value Q(s,a)."""
        if self.visit_count == 0:
            return 0.0
        return self.value_sum / self.visit_count

print("‚úÖ MCTSNode defined!")

# ==============================================================================
# Cell 8: MCTS Algorithm
# ==============================================================================

class MCTS:
    """
    Monte Carlo Tree Search dengan neural network guidance.
    
    Menggunakan PUCT formula untuk balance exploration-exploitation:
    UCB(s,a) = Q(s,a) + c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))
    """
    
    def __init__(
        self,
        network: nn.Module,
        config: AlphaZeroConfig,
        action_space: ActionSpace,
        state_encoder: StateEncoder
    ):
        self.network = network
        self.config = config
        self.action_space = action_space
        self.state_encoder = state_encoder
    
    def search(
        self, 
        board: chess.Board,
        add_noise: bool = True
    ) -> np.ndarray:
        """
        Run MCTS dari posisi saat ini.
        
        Args:
            board: Current board position
            add_noise: Add Dirichlet noise untuk exploration
            
        Returns:
            policy: Visit count probabilities untuk semua actions
        """
        root = MCTSNode(prior=0.0)
        root.to_play = board.turn
        
        # Expand root
        self._expand(root, board)
        
        # Add Dirichlet noise untuk exploration
        if add_noise:
            self._add_dirichlet_noise(root)
        
        # Run simulations
        for _ in range(self.config.num_simulations):
            node = root
            search_path = [node]
            scratch_board = board.copy()
            
            # Selection: traverse tree using UCB
            while node.expanded():
                action, node = self._select_child(node)
                search_path.append(node)
                
                # Apply move to scratch board
                move = self.action_space.decode_action(action)
                if move and move in scratch_board.legal_moves:
                    scratch_board.push(move)
                else:
                    # Find legal move that matches
                    for legal in scratch_board.legal_moves:
                        if self.action_space.encode_move(legal) == action:
                            scratch_board.push(legal)
                            break
                    else:
                        break
                
                # Check terminal
                if scratch_board.is_game_over():
                    break
            
            # Get value
            if scratch_board.is_game_over():
                # Terminal value
                result = scratch_board.result()
                if result == '1-0':
                    value = 1.0 if root.to_play == chess.WHITE else -1.0
                elif result == '0-1':
                    value = -1.0 if root.to_play == chess.WHITE else 1.0
                else:
                    value = 0.0
            else:
                # Expand node and get value from network
                self._expand(node, scratch_board)
                state = self.state_encoder.encode(scratch_board)
                legal_mask = self.action_space.get_legal_action_mask(scratch_board)
                _, value = self.network.get_policy_value(state, legal_mask)
                
                # Flip value for opponent's perspective
                if scratch_board.turn != root.to_play:
                    value = -value
            
            # Backpropagate
            self._backpropagate(search_path, value, root.to_play)
        
        # Return visit count policy
        return self._get_action_policy(root)
    
    def _expand(self, node: MCTSNode, board: chess.Board):
        """Expand node with policy priors from network."""
        state = self.state_encoder.encode(board)
        legal_mask = self.action_space.get_legal_action_mask(board)
        
        policy, _ = self.network.get_policy_value(state, legal_mask)
        
        legal_actions = self.action_space.get_legal_actions(board)
        for action in legal_actions:
            node.children[action] = MCTSNode(prior=policy[action])
            node.children[action].to_play = not board.turn
    
    def _select_child(self, node: MCTSNode) -> Tuple[int, MCTSNode]:
        """Select child node using PUCT formula."""
        total_visits = sum(child.visit_count for child in node.children.values())
        sqrt_total = math.sqrt(total_visits + 1)
        
        best_action = -1
        best_ucb = -float('inf')
        
        for action, child in node.children.items():
            # PUCT formula
            q_value = -child.value()  # Negamax
            exploration = (self.config.c_puct * child.prior * sqrt_total 
                          / (1 + child.visit_count))
            ucb = q_value + exploration
            
            if ucb > best_ucb:
                best_ucb = ucb
                best_action = action
        
        return best_action, node.children[best_action]
    
    def _backpropagate(
        self, 
        search_path: List[MCTSNode], 
        value: float,
        root_player: chess.Color
    ):
        """Backpropagate value up the tree."""
        for node in reversed(search_path):
            node.visit_count += 1
            # Value is from root player's perspective
            if node.to_play == root_player:
                node.value_sum += value
            else:
                node.value_sum -= value
    
    def _add_dirichlet_noise(self, node: MCTSNode):
        """Add Dirichlet noise to root priors for exploration."""
        actions = list(node.children.keys())
        noise = np.random.dirichlet([self.config.dirichlet_alpha] * len(actions))
        
        for i, action in enumerate(actions):
            node.children[action].prior = (
                (1 - self.config.dirichlet_epsilon) * node.children[action].prior +
                self.config.dirichlet_epsilon * noise[i]
            )
    
    def _get_action_policy(self, root: MCTSNode) -> np.ndarray:
        """Get policy from visit counts."""
        policy = np.zeros(self.config.action_size, dtype=np.float32)
        
        total_visits = sum(child.visit_count for child in root.children.values())
        if total_visits > 0:
            for action, child in root.children.items():
                policy[action] = child.visit_count / total_visits
        
        return policy

print("‚úÖ MCTS algorithm defined!")

# ==============================================================================
# Cell 9: Training Buffer
# ==============================================================================

@dataclass
class TrainingExample:
    """Single training example from self-play."""
    state: np.ndarray
    policy: np.ndarray
    value: float

class TrainingBuffer:
    """
    Circular buffer untuk menyimpan training examples dari self-play.
    """
    
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.buffer: deque = deque(maxlen=capacity)
    
    def add(self, example: TrainingExample):
        """Add single example."""
        self.buffer.append(example)
    
    def add_game(self, examples: List[TrainingExample]):
        """Add all examples from a game."""
        for ex in examples:
            self.buffer.append(ex)
    
    def sample(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Sample random batch."""
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
        
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        
        states = np.array([self.buffer[i].state for i in indices])
        policies = np.array([self.buffer[i].policy for i in indices])
        values = np.array([self.buffer[i].value for i in indices])
        
        return states, policies, values
    
    def __len__(self):
        return len(self.buffer)

training_buffer = TrainingBuffer(CONFIG.buffer_size)
print(f"‚úÖ TrainingBuffer created (capacity: {CONFIG.buffer_size:,})")

# ==============================================================================
# Cell 10: Self-Play Game Generation
# ==============================================================================

class SelfPlayGame:
    """Generate training data from self-play."""
    
    def __init__(
        self,
        network: nn.Module,
        config: AlphaZeroConfig,
        action_space: ActionSpace,
        state_encoder: StateEncoder
    ):
        self.network = network
        self.config = config
        self.action_space = action_space
        self.state_encoder = state_encoder
        self.mcts = MCTS(network, config, action_space, state_encoder)
    
    def play_game(self) -> List[TrainingExample]:
        """
        Play one self-play game.
        
        Returns:
            List of training examples (state, mcts_policy, value)
        """
        board = chess.Board()
        examples = []
        move_count = 0
        
        while not board.is_game_over() and move_count < self.config.max_game_length:
            # Get state and legal moves
            state = self.state_encoder.encode(board)
            legal_mask = self.action_space.get_legal_action_mask(board)
            
            # Run MCTS
            add_noise = move_count < 30  # Add noise early in game
            mcts_policy = self.mcts.search(board, add_noise=add_noise)
            
            # Store example (outcome filled in later)
            examples.append(TrainingExample(
                state=state.copy(),
                policy=mcts_policy.copy(),
                value=0.0  # Placeholder
            ))
            
            # Temperature-based action selection
            if move_count < self.config.temperature_drop_move:
                temperature = self.config.temperature_init
            else:
                temperature = self.config.temperature_final
            
            action = self._sample_action(mcts_policy, temperature)
            
            # Convert action to move and play
            move = self.action_space.decode_action(action)
            if move and move in board.legal_moves:
                board.push(move)
            else:
                # Find matching legal move
                for legal in board.legal_moves:
                    if self.action_space.encode_move(legal) == action:
                        board.push(legal)
                        break
                else:
                    # Fallback: random legal move
                    board.push(random.choice(list(board.legal_moves)))
            
            move_count += 1
        
        # Get game outcome
        result = board.result()
        if result == '1-0':
            outcome = 1.0  # White wins
        elif result == '0-1':
            outcome = -1.0  # Black wins
        else:
            outcome = 0.0  # Draw
        
        # Fill in values from perspective of player to move
        for i, ex in enumerate(examples):
            # Alternate perspective
            if i % 2 == 0:  # White's move
                ex.value = outcome
            else:  # Black's move
                ex.value = -outcome
        
        return examples
    
    def _sample_action(self, policy: np.ndarray, temperature: float) -> int:
        """Sample action from policy with temperature."""
        if temperature < 0.01:
            # Greedy
            return int(np.argmax(policy))
        else:
            # Temperature-scaled sampling
            policy_temp = policy ** (1 / temperature)
            policy_temp = policy_temp / policy_temp.sum()
            return int(np.random.choice(len(policy), p=policy_temp))

print("‚úÖ SelfPlayGame defined!")

# ==============================================================================
# Cell 11: AlphaZero Trainer
# ==============================================================================

class AlphaZeroTrainer:
    """
    Complete AlphaZero training pipeline.
    
    Training loop:
    1. Self-play: Generate games with current network
    2. Training: Train network on recent positions
    3. Evaluation: Compare new network with best
    """
    
    def __init__(
        self,
        network: nn.Module,
        config: AlphaZeroConfig,
        device: torch.device
    ):
        self.network = network
        self.config = config
        self.device = device
        
        # Action space and encoder
        self.action_space = ACTION_SPACE
        self.state_encoder = STATE_ENCODER
        
        # Training buffer
        self.buffer = training_buffer
        
        # Self-play generator
        self.self_play = SelfPlayGame(
            network, config, self.action_space, self.state_encoder
        )
        
        # Optimizer
        self.optimizer = torch.optim.AdamW(
            network.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.num_iterations * config.training_steps_per_iter,
            eta_min=1e-6
        )
        
        # Mixed precision
        self.scaler = torch.cuda.amp.GradScaler(enabled=config.use_fp16)
        
        # Best network for evaluation
        self.best_network = copy.deepcopy(network)
        
        # Training history
        self.history = {
            'policy_loss': [], 'value_loss': [], 'total_loss': [],
            'lr': [], 'buffer_size': [], 'games_played': [],
            'win_rate_vs_random': [], 'win_rate_vs_best': []
        }
        
        # Tracking
        self.total_games = 0
        self.iteration = 0
        self.best_win_rate = 0.0
    
    def run_self_play(self, num_games: int) -> int:
        """Run self-play games and add to buffer."""
        self.network.eval()
        
        total_positions = 0
        for _ in tqdm(range(num_games), desc="Self-play", leave=False):
            examples = self.self_play.play_game()
            self.buffer.add_game(examples)
            total_positions += len(examples)
            self.total_games += 1
        
        return total_positions
    
    def train_step(self) -> Dict[str, float]:
        """Single training step on batch from buffer."""
        self.network.train()
        
        # Sample batch
        states, target_policies, target_values = self.buffer.sample(
            self.config.batch_size
        )
        
        # Convert to tensors
        states_t = torch.FloatTensor(states).to(self.device)
        target_policies_t = torch.FloatTensor(target_policies).to(self.device)
        target_values_t = torch.FloatTensor(target_values).to(self.device)
        
        # Get legal masks (assume all moves were legal in training data)
        # For training, we use a soft mask based on target policy
        mask_t = target_policies_t > 0
        
        # Forward pass with mixed precision
        with torch.cuda.amp.autocast(enabled=self.config.use_fp16):
            log_policy, value = self.network(states_t, mask_t)
            
            # Policy loss: cross-entropy
            # -sum(target * log(pred))
            policy_loss = -torch.sum(target_policies_t * log_policy, dim=-1).mean()
            
            # Value loss: MSE
            if self.config.use_wdl:
                # Convert target value to WDL
                # value = -1 -> (0, 0, 1), value = 0 -> (0, 1, 0), value = 1 -> (1, 0, 0)
                target_wdl = torch.zeros_like(value)
                for i, v in enumerate(target_values_t):
                    if v > 0.5:
                        target_wdl[i] = torch.tensor([1.0, 0.0, 0.0])
                    elif v < -0.5:
                        target_wdl[i] = torch.tensor([0.0, 0.0, 1.0])
                    else:
                        target_wdl[i] = torch.tensor([0.0, 1.0, 0.0])
                value_loss = F.mse_loss(value, target_wdl.to(self.device))
            else:
                value_loss = F.mse_loss(value.squeeze(-1), target_values_t)
            
            # Total loss
            total_loss = policy_loss + value_loss
        
        # Backward pass
        self.optimizer.zero_grad()
        self.scaler.scale(total_loss).backward()
        self.scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)
        self.scaler.step(self.optimizer)
        self.scaler.update()
        self.scheduler.step()
        
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'total_loss': total_loss.item()
        }
    
    def evaluate_vs_random(self, num_games: int = 20) -> float:
        """Evaluate network against random player."""
        self.network.eval()
        wins = 0
        
        for _ in range(num_games):
            board = chess.Board()
            move_count = 0
            
            while not board.is_game_over() and move_count < 200:
                if board.turn == chess.WHITE:
                    # Network plays white
                    state = self.state_encoder.encode(board)
                    legal_mask = self.action_space.get_legal_action_mask(board)
                    policy, _ = self.network.get_policy_value(state, legal_mask)
                    action = int(np.argmax(policy))
                    
                    move = self.action_space.decode_action(action)
                    if move and move in board.legal_moves:
                        board.push(move)
                    else:
                        for legal in board.legal_moves:
                            if self.action_space.encode_move(legal) == action:
                                board.push(legal)
                                break
                        else:
                            board.push(random.choice(list(board.legal_moves)))
                else:
                    # Random plays black
                    board.push(random.choice(list(board.legal_moves)))
                
                move_count += 1
            
            result = board.result()
            if result == '1-0':
                wins += 1
        
        return wins / num_games
    
    def arena_evaluate(self) -> float:
        """Evaluate current network against best network."""
        self.network.eval()
        self.best_network.eval()
        
        current_wins = 0
        total_games = self.config.arena_games
        
        for game_idx in range(total_games):
            board = chess.Board()
            move_count = 0
            
            # Alternate colors
            current_is_white = game_idx % 2 == 0
            
            while not board.is_game_over() and move_count < 200:
                state = self.state_encoder.encode(board)
                legal_mask = self.action_space.get_legal_action_mask(board)
                
                if (board.turn == chess.WHITE) == current_is_white:
                    # Current network's turn
                    policy, _ = self.network.get_policy_value(state, legal_mask)
                else:
                    # Best network's turn
                    policy, _ = self.best_network.get_policy_value(state, legal_mask)
                
                action = int(np.argmax(policy))
                move = self.action_space.decode_action(action)
                
                if move and move in board.legal_moves:
                    board.push(move)
                else:
                    for legal in board.legal_moves:
                        if self.action_space.encode_move(legal) == action:
                            board.push(legal)
                            break
                    else:
                        board.push(random.choice(list(board.legal_moves)))
                
                move_count += 1
            
            result = board.result()
            if current_is_white:
                if result == '1-0':
                    current_wins += 1
                elif result == '1/2-1/2':
                    current_wins += 0.5
            else:
                if result == '0-1':
                    current_wins += 1
                elif result == '1/2-1/2':
                    current_wins += 0.5
        
        return current_wins / total_games
    
    def save_checkpoint(self, path: str, is_best: bool = False):
        """Save training checkpoint."""
        checkpoint = {
            'iteration': self.iteration,
            'network_state_dict': self.network.state_dict(),
            'best_network_state_dict': self.best_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'history': self.history,
            'total_games': self.total_games,
            'best_win_rate': self.best_win_rate,
            'config': self.config.__dict__
        }
        torch.save(checkpoint, path)
        
        if is_best:
            best_path = path.replace('.pt', '_best.pt')
            torch.save(checkpoint, best_path)
    
    def load_checkpoint(self, path: str):
        """Load training checkpoint."""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.network.load_state_dict(checkpoint['network_state_dict'])
        self.best_network.load_state_dict(checkpoint['best_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        self.history = checkpoint['history']
        self.iteration = checkpoint['iteration']
        self.total_games = checkpoint['total_games']
        self.best_win_rate = checkpoint['best_win_rate']
        
        print(f"‚úÖ Loaded checkpoint from iteration {self.iteration}")
    
    def train(self, checkpoint_dir: str = '/kaggle/working'):
        """Main training loop."""
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        print("=" * 70)
        print("üèÜ ALPHAZERO CHESS TRAINING")
        print("=" * 70)
        print(f"   Total iterations: {self.config.num_iterations}")
        print(f"   Games per iteration: {self.config.games_per_iteration}")
        print(f"   MCTS simulations: {self.config.num_simulations}")
        print(f"   Buffer size: {self.config.buffer_size:,}")
        print("=" * 70)
        
        start_time = time.time()
        
        for iteration in range(self.iteration, self.config.num_iterations):
            self.iteration = iteration
            iter_start = time.time()
            
            # Phase 1: Self-play
            print(f"\nüìå Iteration {iteration + 1}/{self.config.num_iterations}")
            print("   üéÆ Self-play...")
            
            new_positions = self.run_self_play(self.config.games_per_iteration)
            print(f"      Generated {new_positions} positions, buffer: {len(self.buffer):,}")
            
            # Phase 2: Training
            if len(self.buffer) >= self.config.min_buffer_size:
                print("   üéØ Training...")
                
                total_policy_loss = 0
                total_value_loss = 0
                
                for step in tqdm(range(self.config.training_steps_per_iter), 
                               desc="Training", leave=False):
                    losses = self.train_step()
                    total_policy_loss += losses['policy_loss']
                    total_value_loss += losses['value_loss']
                
                avg_policy_loss = total_policy_loss / self.config.training_steps_per_iter
                avg_value_loss = total_value_loss / self.config.training_steps_per_iter
                
                self.history['policy_loss'].append(avg_policy_loss)
                self.history['value_loss'].append(avg_value_loss)
                self.history['total_loss'].append(avg_policy_loss + avg_value_loss)
                self.history['lr'].append(self.scheduler.get_last_lr()[0])
                self.history['buffer_size'].append(len(self.buffer))
                self.history['games_played'].append(self.total_games)
                
                print(f"      Policy Loss: {avg_policy_loss:.4f}")
                print(f"      Value Loss: {avg_value_loss:.4f}")
                print(f"      LR: {self.scheduler.get_last_lr()[0]:.2e}")
            else:
                print(f"   ‚è≥ Waiting for buffer ({len(self.buffer)}/{self.config.min_buffer_size})")
            
            # Phase 3: Evaluation
            if (iteration + 1) % self.config.eval_interval == 0:
                print("   üìä Evaluating...")
                
                # vs Random
                win_rate_random = self.evaluate_vs_random(self.config.eval_games)
                self.history['win_rate_vs_random'].append(win_rate_random)
                print(f"      vs Random: {win_rate_random:.1%}")
                
                # vs Best (arena)
                win_rate_best = self.arena_evaluate()
                self.history['win_rate_vs_best'].append(win_rate_best)
                print(f"      vs Best: {win_rate_best:.1%}")
                
                # Update best if improved
                if win_rate_best > self.config.arena_threshold:
                    print(f"      üèÜ New best model! ({win_rate_best:.1%} > {self.config.arena_threshold:.1%})")
                    self.best_network = copy.deepcopy(self.network)
                    self.best_win_rate = win_rate_best
                    self.save_checkpoint(
                        f'{checkpoint_dir}/alphazero_best.pt', is_best=True
                    )
            
            # Checkpointing
            if (iteration + 1) % self.config.save_interval == 0:
                self.save_checkpoint(f'{checkpoint_dir}/alphazero_checkpoint.pt')
                print(f"   üíæ Checkpoint saved")
            
            iter_time = time.time() - iter_start
            total_time = time.time() - start_time
            print(f"   ‚è±Ô∏è Iteration: {iter_time:.1f}s | Total: {total_time/60:.1f}min")
        
        # Final save
        self.save_checkpoint(f'{checkpoint_dir}/alphazero_final.pt')
        
        print("\n" + "=" * 70)
        print("üèÅ TRAINING COMPLETE!")
        print("=" * 70)
        print(f"   Total time: {(time.time() - start_time)/3600:.2f} hours")
        print(f"   Total games: {self.total_games:,}")
        print(f"   Best win rate vs Best: {self.best_win_rate:.1%}")

print("‚úÖ AlphaZeroTrainer defined!")

# ==============================================================================
# Cell 12: Visualization
# ==============================================================================

def smooth_curve(values, weight=0.9):
    """Exponential moving average smoothing."""
    smoothed = []
    last = values[0] if values else 0
    for point in values:
        smoothed_val = last * weight + (1 - weight) * point
        smoothed.append(smoothed_val)
        last = smoothed_val
    return smoothed

def plot_training_curves(history, save_path=None):
    """Plot comprehensive training curves."""
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # Policy Loss
    if history['policy_loss']:
        axes[0, 0].plot(history['policy_loss'], alpha=0.3, color='blue')
        axes[0, 0].plot(smooth_curve(history['policy_loss']), color='blue', linewidth=2)
        axes[0, 0].set_title('Policy Loss (Cross-Entropy)')
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].grid(True, alpha=0.3)
    
    # Value Loss
    if history['value_loss']:
        axes[0, 1].plot(history['value_loss'], alpha=0.3, color='orange')
        axes[0, 1].plot(smooth_curve(history['value_loss']), color='orange', linewidth=2)
        axes[0, 1].set_title('Value Loss (MSE)')
        axes[0, 1].set_xlabel('Iteration')
        axes[0, 1].grid(True, alpha=0.3)
    
    # Learning Rate
    if history['lr']:
        axes[0, 2].plot(history['lr'], color='green', linewidth=2)
        axes[0, 2].set_title('Learning Rate')
        axes[0, 2].set_xlabel('Iteration')
        axes[0, 2].set_yscale('log')
        axes[0, 2].grid(True, alpha=0.3)
    
    # Win Rate vs Random
    if history['win_rate_vs_random']:
        x = list(range(0, len(history['win_rate_vs_random']) * CONFIG.eval_interval, 
                      CONFIG.eval_interval))
        axes[1, 0].plot(x, history['win_rate_vs_random'], 'go-', markersize=6)
        axes[1, 0].fill_between(x, 0, history['win_rate_vs_random'], alpha=0.3, color='green')
        axes[1, 0].set_title('Win Rate vs Random')
        axes[1, 0].set_xlabel('Iteration')
        axes[1, 0].set_ylim(0, 1)
        axes[1, 0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
        axes[1, 0].grid(True, alpha=0.3)
    
    # Win Rate vs Best
    if history['win_rate_vs_best']:
        x = list(range(0, len(history['win_rate_vs_best']) * CONFIG.eval_interval, 
                      CONFIG.eval_interval))
        axes[1, 1].plot(x, history['win_rate_vs_best'], 'bo-', markersize=6)
        axes[1, 1].axhline(y=CONFIG.arena_threshold, color='r', linestyle='--', 
                          label=f'Threshold ({CONFIG.arena_threshold:.0%})')
        axes[1, 1].set_title('Win Rate vs Best (Arena)')
        axes[1, 1].set_xlabel('Iteration')
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    # Buffer Size
    if history['buffer_size']:
        axes[1, 2].plot(history['buffer_size'], color='purple', linewidth=2)
        axes[1, 2].axhline(y=CONFIG.buffer_size, color='r', linestyle='--', 
                          label='Max capacity')
        axes[1, 2].set_title('Buffer Size')
        axes[1, 2].set_xlabel('Iteration')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"üìä Training curves saved to {save_path}")
    
    plt.show()

print("‚úÖ Visualization functions defined!")

# ==============================================================================
# Cell 13: Main Training Execution
# ==============================================================================

# Create trainer
trainer = AlphaZeroTrainer(network, CONFIG, device)

# Checkpoint directory
CHECKPOINT_DIR = '/kaggle/working'
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# Optional: Resume from checkpoint
# trainer.load_checkpoint(f'{CHECKPOINT_DIR}/alphazero_checkpoint.pt')

print("\n" + "=" * 70)
print("üöÄ STARTING ALPHAZERO TRAINING")
print("=" * 70)

# Run training
trainer.train(checkpoint_dir=CHECKPOINT_DIR)

# ==============================================================================
# Cell 14: Plot Results
# ==============================================================================

print("\nüìä Plotting training curves...")
plot_training_curves(
    trainer.history, 
    save_path=f'{CHECKPOINT_DIR}/training_curves.png'
)

# ==============================================================================
# Cell 15: Final Evaluation
# ==============================================================================

print("\n" + "=" * 70)
print("üìä FINAL EVALUATION")
print("=" * 70)

# Evaluate vs Random (100 games)
print("\nüé≤ Evaluating vs Random (100 games)...")
final_win_rate = trainer.evaluate_vs_random(100)
print(f"   Final Win Rate vs Random: {final_win_rate:.1%}")

# Show best model stats
print(f"\nüìà Training Statistics:")
print(f"   Total iterations: {trainer.iteration + 1}")
print(f"   Total games played: {trainer.total_games:,}")
print(f"   Total positions in buffer: {len(trainer.buffer):,}")
print(f"   Best win rate vs previous: {trainer.best_win_rate:.1%}")

if trainer.history['policy_loss']:
    print(f"   Final policy loss: {trainer.history['policy_loss'][-1]:.4f}")
    print(f"   Final value loss: {trainer.history['value_loss'][-1]:.4f}")

# ==============================================================================
# Cell 16: Demo Game
# ==============================================================================

def play_demo_game(network, verbose=True):
    """Play a demo game to show network's playing style."""
    print("\n" + "=" * 70)
    print("üéÆ DEMO GAME: AI (White) vs Random (Black)")
    print("=" * 70)
    
    network.eval()
    board = chess.Board()
    moves = []
    
    while not board.is_game_over() and len(moves) < 100:
        if board.turn == chess.WHITE:
            # AI move
            state = STATE_ENCODER.encode(board)
            legal_mask = ACTION_SPACE.get_legal_action_mask(board)
            policy, value = network.get_policy_value(state, legal_mask)
            action = int(np.argmax(policy))
            
            move = ACTION_SPACE.decode_action(action)
            if move and move in board.legal_moves:
                san = board.san(move)
                board.push(move)
            else:
                for legal in board.legal_moves:
                    if ACTION_SPACE.encode_move(legal) == action:
                        san = board.san(legal)
                        board.push(legal)
                        break
                else:
                    move = random.choice(list(board.legal_moves))
                    san = board.san(move)
                    board.push(move)
        else:
            # Random move
            move = random.choice(list(board.legal_moves))
            san = board.san(move)
            board.push(move)
        
        moves.append(san)
        
        if verbose and len(moves) % 2 == 0:
            move_num = len(moves) // 2
            white_move = moves[-2] if len(moves) >= 2 else ""
            black_move = moves[-1]
            print(f"   {move_num}. {white_move} {black_move}")
    
    print(f"\n   Result: {board.result()}")
    print(f"   Total moves: {len(moves)}")
    
    return board.result(), moves

demo_result, demo_moves = play_demo_game(trainer.best_network)

# ==============================================================================
# Cell 17: Export Model
# ==============================================================================

def export_model(network, path, config):
    """Export model for inference."""
    export_dict = {
        'network_state_dict': network.state_dict(),
        'config': config.__dict__,
        'num_params': sum(p.numel() for p in network.parameters()),
    }
    torch.save(export_dict, path)
    print(f"‚úÖ Model exported to: {path}")

export_model(
    trainer.best_network, 
    f'{CHECKPOINT_DIR}/alphazero_export.pt',
    CONFIG
)

print("\n" + "=" * 70)
print("üéâ ALL DONE!")
print("=" * 70)
print(f"Files saved in {CHECKPOINT_DIR}:")
print(f"   üìÅ alphazero_best.pt - Best model checkpoint")
print(f"   üìÅ alphazero_final.pt - Final model checkpoint")
print(f"   üìÅ alphazero_export.pt - Exported model for inference")
print(f"   üìä training_curves.png - Training visualization")
print("=" * 70)
