{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèÜ Advanced Chess RL Training\n",
                "\n",
                "## Notebook untuk Training AI Catur yang Kuat\n",
                "\n",
                "**Target:** Mengalahkan Stockfish pada level rendah-menengah\n",
                "\n",
                "### ‚ö†Ô∏è Catatan Penting:\n",
                "- Training membutuhkan **beberapa jam** GPU time\n",
                "- Model akan di-checkpoint setiap beberapa iterasi\n",
                "- Untuk hasil terbaik, jalankan training **berulang kali**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ GPU: Tesla T4\n",
                        "   Memory: 15.8 GB\n"
                    ]
                }
            ],
            "source": [
                "# Check GPU\n",
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
                "    print(f\"   Memory: {gpu_mem:.1f} GB\")\n",
                "    device = torch.device('cuda')\n",
                "else:\n",
                "    print(\"‚ùå GPU tidak tersedia! Training akan sangat lambat.\")\n",
                "    device = torch.device('cpu')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
                        "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "‚úÖ Dependencies installed!\n"
                    ]
                }
            ],
            "source": [
                "# Install dependencies\n",
                "!pip install -q python-chess gymnasium tqdm matplotlib stockfish\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Selecting previously unselected package stockfish.\n",
                        "(Reading database ... 121689 files and directories currently installed.)\n",
                        "Preparing to unpack .../stockfish_14.1-1_amd64.deb ...\n",
                        "Unpacking stockfish (14.1-1) ...\n",
                        "Setting up stockfish (14.1-1) ...\n",
                        "Processing triggers for man-db (2.10.2-1) ...\n",
                        "‚úÖ Stockfish installed!\n"
                    ]
                }
            ],
            "source": [
                "# Install Stockfish untuk evaluasi\n",
                "!apt-get install -qq stockfish\n",
                "print(\"‚úÖ Stockfish installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Chess Environment (Enhanced)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ ChessEnv with reward shaping defined!\n"
                    ]
                }
            ],
            "source": [
                "import chess\n",
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "import random\n",
                "\n",
                "class ChessEnv(gym.Env):\n",
                "    \"\"\"Enhanced Chess Environment dengan reward shaping.\"\"\"\n",
                "    \n",
                "    # Piece values untuk reward shaping\n",
                "    PIECE_VALUES = {\n",
                "        chess.PAWN: 1,\n",
                "        chess.KNIGHT: 3,\n",
                "        chess.BISHOP: 3,\n",
                "        chess.ROOK: 5,\n",
                "        chess.QUEEN: 9,\n",
                "        chess.KING: 0\n",
                "    }\n",
                "    \n",
                "    def __init__(self, max_moves=200, reward_shaping=True):\n",
                "        super().__init__()\n",
                "        self.board = chess.Board()\n",
                "        self.max_moves = max_moves\n",
                "        self.move_count = 0\n",
                "        self.reward_shaping = reward_shaping\n",
                "        self.prev_material = 0\n",
                "        \n",
                "        # State: 18 channels x 8 x 8\n",
                "        # 12 piece planes + turn + castling (4) + en passant\n",
                "        self.observation_space = spaces.Box(\n",
                "            low=0, high=1, shape=(18, 8, 8), dtype=np.float32\n",
                "        )\n",
                "        self.action_space = spaces.Discrete(4672)\n",
                "        self._init_move_encoding()\n",
                "    \n",
                "    def _init_move_encoding(self):\n",
                "        self.action_to_move = {}\n",
                "        self.move_to_action = {}\n",
                "        \n",
                "        directions = []\n",
                "        for d in [(0,1), (0,-1), (1,0), (-1,0), (1,1), (1,-1), (-1,1), (-1,-1)]:\n",
                "            for dist in range(1, 8):\n",
                "                directions.append((d[0]*dist, d[1]*dist))\n",
                "        for d in [(1,2), (2,1), (2,-1), (1,-2), (-1,-2), (-2,-1), (-2,1), (-1,2)]:\n",
                "            directions.append(d)\n",
                "        \n",
                "        action = 0\n",
                "        for sq in range(64):\n",
                "            from_rank, from_file = sq // 8, sq % 8\n",
                "            for dx, dy in directions:\n",
                "                to_rank = from_rank + dy\n",
                "                to_file = from_file + dx\n",
                "                if 0 <= to_rank < 8 and 0 <= to_file < 8:\n",
                "                    to_sq = to_rank * 8 + to_file\n",
                "                    move = chess.Move(sq, to_sq)\n",
                "                    self.action_to_move[action] = move\n",
                "                    self.move_to_action[move.uci()] = action\n",
                "                action += 1\n",
                "            if from_rank == 6:\n",
                "                for dx in [-1, 0, 1]:\n",
                "                    for promo in [chess.KNIGHT, chess.BISHOP, chess.ROOK]:\n",
                "                        to_file = from_file + dx\n",
                "                        if 0 <= to_file < 8:\n",
                "                            to_sq = 7 * 8 + to_file\n",
                "                            move = chess.Move(sq, to_sq, promotion=promo)\n",
                "                            self.action_to_move[action] = move\n",
                "                            self.move_to_action[move.uci()] = action\n",
                "                        action += 1\n",
                "    \n",
                "    def _get_material_score(self, color):\n",
                "        score = 0\n",
                "        for piece_type in self.PIECE_VALUES:\n",
                "            score += len(self.board.pieces(piece_type, color)) * self.PIECE_VALUES[piece_type]\n",
                "        return score\n",
                "    \n",
                "    def encode_state(self):\n",
                "        state = np.zeros((18, 8, 8), dtype=np.float32)\n",
                "        \n",
                "        piece_to_channel = {\n",
                "            (chess.PAWN, True): 0, (chess.KNIGHT, True): 1, (chess.BISHOP, True): 2,\n",
                "            (chess.ROOK, True): 3, (chess.QUEEN, True): 4, (chess.KING, True): 5,\n",
                "            (chess.PAWN, False): 6, (chess.KNIGHT, False): 7, (chess.BISHOP, False): 8,\n",
                "            (chess.ROOK, False): 9, (chess.QUEEN, False): 10, (chess.KING, False): 11\n",
                "        }\n",
                "        \n",
                "        for sq in chess.SQUARES:\n",
                "            piece = self.board.piece_at(sq)\n",
                "            if piece:\n",
                "                rank, file = sq // 8, sq % 8\n",
                "                ch = piece_to_channel[(piece.piece_type, piece.color)]\n",
                "                state[ch, rank, file] = 1.0\n",
                "        \n",
                "        state[12, :, :] = 1.0 if self.board.turn else 0.0\n",
                "        state[13, 0, :] = float(self.board.has_kingside_castling_rights(True))\n",
                "        state[14, 0, :] = float(self.board.has_queenside_castling_rights(True))\n",
                "        state[15, 0, :] = float(self.board.has_kingside_castling_rights(False))\n",
                "        state[16, 0, :] = float(self.board.has_queenside_castling_rights(False))\n",
                "        \n",
                "        if self.board.ep_square:\n",
                "            ep_rank, ep_file = self.board.ep_square // 8, self.board.ep_square % 8\n",
                "            state[17, ep_rank, ep_file] = 1.0\n",
                "        \n",
                "        return state\n",
                "    \n",
                "    def get_legal_action_mask(self):\n",
                "        mask = np.zeros(4672, dtype=bool)\n",
                "        for move in self.board.legal_moves:\n",
                "            uci = move.uci()\n",
                "            if uci in self.move_to_action:\n",
                "                mask[self.move_to_action[uci]] = True\n",
                "        return mask\n",
                "    \n",
                "    def reset(self, seed=None, options=None):\n",
                "        super().reset(seed=seed)\n",
                "        self.board = chess.Board()\n",
                "        self.move_count = 0\n",
                "        white_material = self._get_material_score(True)\n",
                "        black_material = self._get_material_score(False)\n",
                "        self.prev_material = white_material - black_material\n",
                "        return self.encode_state(), {}\n",
                "    \n",
                "    def step(self, action):\n",
                "        if action in self.action_to_move:\n",
                "            move = self.action_to_move[action]\n",
                "            if move in self.board.legal_moves:\n",
                "                self.board.push(move)\n",
                "                self.move_count += 1\n",
                "            else:\n",
                "                for legal in self.board.legal_moves:\n",
                "                    if legal.uci()[:4] == move.uci()[:4]:\n",
                "                        self.board.push(legal)\n",
                "                        self.move_count += 1\n",
                "                        break\n",
                "                else:\n",
                "                    return self.encode_state(), -1.0, True, False, {'illegal': True}\n",
                "        else:\n",
                "            return self.encode_state(), -1.0, True, False, {'illegal': True}\n",
                "        \n",
                "        reward = 0.0\n",
                "        terminated = False\n",
                "        truncated = False\n",
                "        \n",
                "        if self.board.is_checkmate():\n",
                "            reward = 1.0 if not self.board.turn else -1.0\n",
                "            terminated = True\n",
                "        elif self.board.is_game_over():\n",
                "            reward = 0.0\n",
                "            terminated = True\n",
                "        elif self.move_count >= self.max_moves:\n",
                "            truncated = True\n",
                "        elif self.reward_shaping:\n",
                "            # Reward shaping berdasarkan material\n",
                "            white_material = self._get_material_score(True)\n",
                "            black_material = self._get_material_score(False)\n",
                "            current_material = white_material - black_material\n",
                "            material_diff = current_material - self.prev_material\n",
                "            self.prev_material = current_material\n",
                "            \n",
                "            # Small bonus for capturing pieces\n",
                "            if material_diff != 0:\n",
                "                reward = material_diff * 0.01\n",
                "            \n",
                "            # Bonus for check\n",
                "            if self.board.is_check():\n",
                "                reward += 0.01\n",
                "        \n",
                "        return self.encode_state(), reward, terminated, truncated, {}\n",
                "\n",
                "print(\"‚úÖ ChessEnv with reward shaping defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Neural Network (Larger)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Enhanced ChessNetwork with SE blocks defined!\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class SEBlock(nn.Module):\n",
                "    \"\"\"Squeeze-and-Excitation block untuk attention.\"\"\"\n",
                "    def __init__(self, channels, reduction=4):\n",
                "        super().__init__()\n",
                "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
                "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        b, c, _, _ = x.size()\n",
                "        y = self.pool(x).view(b, c)\n",
                "        y = F.relu(self.fc1(y))\n",
                "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
                "        return x * y\n",
                "\n",
                "class ResidualBlock(nn.Module):\n",
                "    \"\"\"Residual block dengan SE attention.\"\"\"\n",
                "    def __init__(self, channels, use_se=True):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
                "        self.bn1 = nn.BatchNorm2d(channels)\n",
                "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
                "        self.bn2 = nn.BatchNorm2d(channels)\n",
                "        self.se = SEBlock(channels) if use_se else nn.Identity()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        residual = x\n",
                "        out = F.relu(self.bn1(self.conv1(x)))\n",
                "        out = self.bn2(self.conv2(out))\n",
                "        out = self.se(out)\n",
                "        return F.relu(out + residual)\n",
                "\n",
                "class ChessNetwork(nn.Module):\n",
                "    \"\"\"Policy-Value Network yang lebih besar untuk training kuat.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_channels=18, num_filters=256, num_blocks=12, action_size=4672):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.action_size = action_size\n",
                "        \n",
                "        # Input conv\n",
                "        self.input_conv = nn.Sequential(\n",
                "            nn.Conv2d(input_channels, num_filters, 3, padding=1, bias=False),\n",
                "            nn.BatchNorm2d(num_filters),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        \n",
                "        # Residual tower dengan SE blocks\n",
                "        self.res_blocks = nn.ModuleList([\n",
                "            ResidualBlock(num_filters, use_se=(i % 2 == 0))\n",
                "            for i in range(num_blocks)\n",
                "        ])\n",
                "        \n",
                "        # Policy head \n",
                "        self.policy_head = nn.Sequential(\n",
                "            nn.Conv2d(num_filters, 80, 1, bias=False),\n",
                "            nn.BatchNorm2d(80),\n",
                "            nn.ReLU(),\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(80 * 64, action_size)\n",
                "        )\n",
                "        \n",
                "        # Value head\n",
                "        self.value_head = nn.Sequential(\n",
                "            nn.Conv2d(num_filters, 32, 1, bias=False),\n",
                "            nn.BatchNorm2d(32),\n",
                "            nn.ReLU(),\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(32 * 64, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 1),\n",
                "            nn.Tanh()\n",
                "        )\n",
                "    \n",
                "    def forward(self, x, legal_mask=None):\n",
                "        x = self.input_conv(x)\n",
                "        for block in self.res_blocks:\n",
                "            x = block(x)\n",
                "        \n",
                "        policy_logits = self.policy_head(x)\n",
                "        \n",
                "        if legal_mask is not None:\n",
                "            policy_logits = policy_logits.masked_fill(~legal_mask, float('-inf'))\n",
                "        \n",
                "        log_probs = F.log_softmax(policy_logits, dim=-1)\n",
                "        value = self.value_head(x)\n",
                "        \n",
                "        return log_probs, value\n",
                "\n",
                "print(\"‚úÖ Enhanced ChessNetwork with SE blocks defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Create Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìã Configuration:\n",
                        "   input_channels: 18\n",
                        "   num_filters: 256\n",
                        "   num_blocks: 12\n",
                        "   gamma: 0.99\n",
                        "   gae_lambda: 0.95\n",
                        "   clip_range: 0.2\n",
                        "   entropy_coef: 0.02\n",
                        "   value_coef: 0.5\n",
                        "   max_grad_norm: 0.5\n",
                        "   learning_rate: 0.0001\n",
                        "   n_steps: 512\n",
                        "   n_epochs: 4\n",
                        "   batch_size: 128\n",
                        "   total_updates: 5000\n",
                        "   save_interval: 500\n",
                        "   eval_interval: 250\n"
                    ]
                }
            ],
            "source": [
                "# Configuration - ADVANCED TRAINING\n",
                "CONFIG = {\n",
                "    # Network\n",
                "    'input_channels': 18,\n",
                "    'num_filters': 256,   # Lebih besar: 256 (vs 128)\n",
                "    'num_blocks': 12,     # Lebih dalam: 12 (vs 6)\n",
                "    \n",
                "    # PPO\n",
                "    'gamma': 0.99,\n",
                "    'gae_lambda': 0.95,\n",
                "    'clip_range': 0.2,\n",
                "    'entropy_coef': 0.02,   # Lebih tinggi untuk eksplorasi\n",
                "    'value_coef': 0.5,\n",
                "    'max_grad_norm': 0.5,\n",
                "    \n",
                "    # Training\n",
                "    'learning_rate': 1e-4,   # Lebih kecil untuk stabilitas\n",
                "    'n_steps': 512,          # Lebih panjang: 512 (vs 128)\n",
                "    'n_epochs': 4,\n",
                "    'batch_size': 128,       # Lebih besar: 128 (vs 64)\n",
                "    'total_updates': 5000,   # JAUH lebih banyak: 5000 (vs 100)\n",
                "    \n",
                "    # Checkpointing\n",
                "    'save_interval': 500,    # Save setiap 500 updates\n",
                "    'eval_interval': 250,    # Evaluate setiap 250 updates\n",
                "}\n",
                "\n",
                "print(\"üìã Configuration:\")\n",
                "for k, v in CONFIG.items():\n",
                "    print(f\"   {k}: {v}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Environment: obs=(18, 8, 8), actions=4672\n",
                        "‚úÖ Network: 38,887,585 parameters\n",
                        "‚úÖ Optimizer: AdamW with cosine annealing LR\n"
                    ]
                }
            ],
            "source": [
                "# Create environment & network\n",
                "env = ChessEnv(max_moves=200, reward_shaping=True)\n",
                "print(f\"‚úÖ Environment: obs={env.observation_space.shape}, actions={env.action_space.n}\")\n",
                "\n",
                "network = ChessNetwork(\n",
                "    input_channels=CONFIG['input_channels'],\n",
                "    num_filters=CONFIG['num_filters'],\n",
                "    num_blocks=CONFIG['num_blocks'],\n",
                "    action_size=4672\n",
                ").to(device)\n",
                "\n",
                "num_params = sum(p.numel() for p in network.parameters())\n",
                "print(f\"‚úÖ Network: {num_params:,} parameters\")\n",
                "\n",
                "# Optimizer dengan weight decay untuk regularization\n",
                "optimizer = torch.optim.AdamW(\n",
                "    network.parameters(), \n",
                "    lr=CONFIG['learning_rate'],\n",
                "    weight_decay=1e-4\n",
                ")\n",
                "\n",
                "# Learning rate scheduler\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
                "    optimizer, \n",
                "    T_max=CONFIG['total_updates'],\n",
                "    eta_min=1e-6\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Optimizer: AdamW with cosine annealing LR\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ PPO Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Training functions defined!\n"
                    ]
                }
            ],
            "source": [
                "from torch.distributions import Categorical\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import time\n",
                "\n",
                "# Training history\n",
                "history = {\n",
                "    'policy_loss': [], 'value_loss': [], 'entropy': [], \n",
                "    'rewards': [], 'game_lengths': [], 'lr': []\n",
                "}\n",
                "\n",
                "def select_action(state, legal_mask):\n",
                "    network.eval()\n",
                "    with torch.no_grad():\n",
                "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "        mask_t = torch.BoolTensor(legal_mask).unsqueeze(0).to(device)\n",
                "        log_probs, value = network(state_t, mask_t)\n",
                "        probs = torch.exp(log_probs)\n",
                "        dist = Categorical(probs)\n",
                "        action = dist.sample()\n",
                "    network.train()\n",
                "    return action.item(), log_probs[0, action.item()].item(), value.item()\n",
                "\n",
                "def compute_gae(rewards, values, dones, last_value):\n",
                "    gamma = CONFIG['gamma']\n",
                "    lam = CONFIG['gae_lambda']\n",
                "    advantages = np.zeros_like(rewards)\n",
                "    last_gae = 0\n",
                "    for t in reversed(range(len(rewards))):\n",
                "        if t == len(rewards) - 1:\n",
                "            next_value = last_value\n",
                "        else:\n",
                "            next_value = values[t + 1]\n",
                "        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]\n",
                "        advantages[t] = last_gae = delta + gamma * lam * (1 - dones[t]) * last_gae\n",
                "    returns = advantages + np.array(values)\n",
                "    return advantages, returns\n",
                "\n",
                "def evaluate_vs_random(n_games=20):\n",
                "    \"\"\"Evaluasi melawan random player.\"\"\"\n",
                "    network.eval()\n",
                "    wins = 0\n",
                "    draws = 0\n",
                "    \n",
                "    for _ in range(n_games):\n",
                "        state, _ = env.reset()\n",
                "        done = False\n",
                "        \n",
                "        while not done:\n",
                "            legal_mask = env.get_legal_action_mask()\n",
                "            \n",
                "            if env.board.turn:  # AI plays white\n",
                "                action, _, _ = select_action(state, legal_mask)\n",
                "            else:  # Random plays black\n",
                "                legal_actions = np.where(legal_mask)[0]\n",
                "                action = np.random.choice(legal_actions)\n",
                "            \n",
                "            state, _, terminated, truncated, _ = env.step(action)\n",
                "            done = terminated or truncated\n",
                "        \n",
                "        result = env.board.result()\n",
                "        if result == '1-0':\n",
                "            wins += 1\n",
                "        elif result == '1/2-1/2':\n",
                "            draws += 1\n",
                "    \n",
                "    network.train()\n",
                "    return wins / n_games, draws / n_games\n",
                "\n",
                "print(\"‚úÖ Training functions defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "üöÄ STARTING ADVANCED PPO TRAINING\n",
                        "============================================================\n",
                        "   Total updates: 5000\n",
                        "   Steps per update: 512\n",
                        "   Estimated time: 41.7 - 166.7 minutes\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training:   2%|‚ñè         | 85/5000 [06:43<6:28:56,  4.75s/it]"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# MAIN TRAINING LOOP\n",
                "# ============================================================\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üöÄ STARTING ADVANCED PPO TRAINING\")\n",
                "print(\"=\"*60)\n",
                "print(f\"   Total updates: {CONFIG['total_updates']}\")\n",
                "print(f\"   Steps per update: {CONFIG['n_steps']}\")\n",
                "print(f\"   Estimated time: {CONFIG['total_updates'] * 0.5 / 60:.1f} - {CONFIG['total_updates'] * 2 / 60:.1f} minutes\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "start_time = time.time()\n",
                "best_win_rate = 0.0\n",
                "\n",
                "for update in tqdm(range(CONFIG['total_updates']), desc=\"Training\"):\n",
                "    # Collect rollout\n",
                "    states, actions, rewards, dones = [], [], [], []\n",
                "    old_log_probs, values, masks = [], [], []\n",
                "    \n",
                "    state, _ = env.reset()\n",
                "    episode_reward = 0\n",
                "    episode_rewards = []\n",
                "    episode_lengths = []\n",
                "    episode_length = 0\n",
                "    \n",
                "    for step in range(CONFIG['n_steps']):\n",
                "        legal_mask = env.get_legal_action_mask()\n",
                "        action, log_prob, value = select_action(state, legal_mask)\n",
                "        \n",
                "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "        done = terminated or truncated\n",
                "        \n",
                "        states.append(state)\n",
                "        actions.append(action)\n",
                "        rewards.append(reward)\n",
                "        dones.append(done)\n",
                "        old_log_probs.append(log_prob)\n",
                "        values.append(value)\n",
                "        masks.append(legal_mask)\n",
                "        \n",
                "        episode_reward += reward\n",
                "        episode_length += 1\n",
                "        \n",
                "        if done:\n",
                "            episode_rewards.append(episode_reward)\n",
                "            episode_lengths.append(episode_length)\n",
                "            episode_reward = 0\n",
                "            episode_length = 0\n",
                "            state, _ = env.reset()\n",
                "        else:\n",
                "            state = next_state\n",
                "    \n",
                "    # Compute last value\n",
                "    with torch.no_grad():\n",
                "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "        _, last_value = network(state_t)\n",
                "        last_value = last_value.item()\n",
                "    \n",
                "    # Compute GAE\n",
                "    advantages, returns = compute_gae(rewards, values, dones, last_value)\n",
                "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
                "    \n",
                "    # Convert to tensors\n",
                "    states_t = torch.FloatTensor(np.array(states)).to(device)\n",
                "    actions_t = torch.LongTensor(actions).to(device)\n",
                "    old_log_probs_t = torch.FloatTensor(old_log_probs).to(device)\n",
                "    advantages_t = torch.FloatTensor(advantages).to(device)\n",
                "    returns_t = torch.FloatTensor(returns).to(device)\n",
                "    masks_t = torch.BoolTensor(np.array(masks)).to(device)\n",
                "    \n",
                "    # PPO update\n",
                "    all_policy_loss, all_value_loss, all_entropy = [], [], []\n",
                "    \n",
                "    for epoch in range(CONFIG['n_epochs']):\n",
                "        indices = np.random.permutation(len(states))\n",
                "        for start in range(0, len(states), CONFIG['batch_size']):\n",
                "            end = start + CONFIG['batch_size']\n",
                "            batch_idx = indices[start:end]\n",
                "            \n",
                "            log_probs, values_pred = network(states_t[batch_idx], masks_t[batch_idx])\n",
                "            values_pred = values_pred.squeeze(-1)\n",
                "            \n",
                "            action_log_probs = log_probs.gather(1, actions_t[batch_idx].unsqueeze(-1)).squeeze(-1)\n",
                "            \n",
                "            # Policy loss\n",
                "            ratio = torch.exp(action_log_probs - old_log_probs_t[batch_idx])\n",
                "            surr1 = ratio * advantages_t[batch_idx]\n",
                "            surr2 = torch.clamp(ratio, 1 - CONFIG['clip_range'], 1 + CONFIG['clip_range']) * advantages_t[batch_idx]\n",
                "            policy_loss = -torch.min(surr1, surr2).mean()\n",
                "            \n",
                "            # Value loss\n",
                "            value_loss = F.mse_loss(values_pred, returns_t[batch_idx])\n",
                "            \n",
                "            # Entropy\n",
                "            probs = torch.exp(log_probs)\n",
                "            entropy = -(probs * log_probs.masked_fill(torch.isinf(log_probs), 0)).sum(-1).mean()\n",
                "            \n",
                "            # Total loss\n",
                "            loss = policy_loss + CONFIG['value_coef'] * value_loss - CONFIG['entropy_coef'] * entropy\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(network.parameters(), CONFIG['max_grad_norm'])\n",
                "            optimizer.step()\n",
                "            \n",
                "            all_policy_loss.append(policy_loss.item())\n",
                "            all_value_loss.append(value_loss.item())\n",
                "            all_entropy.append(entropy.item())\n",
                "    \n",
                "    scheduler.step()\n",
                "    \n",
                "    # Record history\n",
                "    history['policy_loss'].append(np.mean(all_policy_loss))\n",
                "    history['value_loss'].append(np.mean(all_value_loss))\n",
                "    history['entropy'].append(np.mean(all_entropy))\n",
                "    history['rewards'].append(np.mean(episode_rewards) if episode_rewards else 0)\n",
                "    history['game_lengths'].append(np.mean(episode_lengths) if episode_lengths else 0)\n",
                "    history['lr'].append(scheduler.get_last_lr()[0])\n",
                "    \n",
                "    # Logging\n",
                "    if (update + 1) % 100 == 0:\n",
                "        elapsed = time.time() - start_time\n",
                "        print(f\"\\nUpdate {update+1}/{CONFIG['total_updates']} | Time: {elapsed/60:.1f}min\")\n",
                "        print(f\"  PolicyL: {history['policy_loss'][-1]:.4f} | ValueL: {history['value_loss'][-1]:.4f}\")\n",
                "        print(f\"  Entropy: {history['entropy'][-1]:.4f} | LR: {history['lr'][-1]:.2e}\")\n",
                "    \n",
                "    # Evaluation\n",
                "    if (update + 1) % CONFIG['eval_interval'] == 0:\n",
                "        win_rate, draw_rate = evaluate_vs_random(20)\n",
                "        print(f\"  üìä vs Random: Win={win_rate:.1%}, Draw={draw_rate:.1%}\")\n",
                "        \n",
                "        if win_rate > best_win_rate:\n",
                "            best_win_rate = win_rate\n",
                "            torch.save({\n",
                "                'update': update,\n",
                "                'network_state_dict': network.state_dict(),\n",
                "                'optimizer_state_dict': optimizer.state_dict(),\n",
                "                'best_win_rate': best_win_rate,\n",
                "            }, '/content/chess_model_best.pt')\n",
                "            print(f\"  üíæ New best model saved! (win_rate={best_win_rate:.1%})\")\n",
                "    \n",
                "    # Checkpointing\n",
                "    if (update + 1) % CONFIG['save_interval'] == 0:\n",
                "        torch.save({\n",
                "            'update': update,\n",
                "            'network_state_dict': network.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'scheduler_state_dict': scheduler.state_dict(),\n",
                "            'history': history,\n",
                "            'config': CONFIG,\n",
                "        }, f'/content/chess_checkpoint_{update+1}.pt')\n",
                "        print(f\"  üíæ Checkpoint saved!\")\n",
                "\n",
                "total_time = time.time() - start_time\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"‚úÖ TRAINING COMPLETED!\")\n",
                "print(f\"   Total time: {total_time/3600:.2f} hours\")\n",
                "print(f\"   Best win rate vs random: {best_win_rate:.1%}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Training Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "\n",
                "# Smoothing function\n",
                "def smooth(data, window=50):\n",
                "    if len(data) < window:\n",
                "        return data\n",
                "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
                "\n",
                "axes[0, 0].plot(smooth(history['policy_loss']))\n",
                "axes[0, 0].set_title('Policy Loss')\n",
                "axes[0, 0].set_xlabel('Update')\n",
                "\n",
                "axes[0, 1].plot(smooth(history['value_loss']))\n",
                "axes[0, 1].set_title('Value Loss')\n",
                "axes[0, 1].set_xlabel('Update')\n",
                "\n",
                "axes[0, 2].plot(smooth(history['entropy']))\n",
                "axes[0, 2].set_title('Entropy')\n",
                "axes[0, 2].set_xlabel('Update')\n",
                "\n",
                "axes[1, 0].plot(smooth(history['rewards']))\n",
                "axes[1, 0].set_title('Mean Episode Reward')\n",
                "axes[1, 0].set_xlabel('Update')\n",
                "\n",
                "axes[1, 1].plot(smooth(history['game_lengths']))\n",
                "axes[1, 1].set_title('Mean Game Length')\n",
                "axes[1, 1].set_xlabel('Update')\n",
                "\n",
                "axes[1, 2].plot(history['lr'])\n",
                "axes[1, 2].set_title('Learning Rate')\n",
                "axes[1, 2].set_xlabel('Update')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/training_curves.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Evaluasi vs Stockfish"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from stockfish import Stockfish\n",
                "\n",
                "def evaluate_vs_stockfish(model, stockfish_path='/usr/games/stockfish', \n",
                "                          skill_level=0, n_games=10, time_limit=0.1):\n",
                "    \"\"\"\n",
                "    Evaluasi model melawan Stockfish.\n",
                "    skill_level: 0-20 (0 = paling lemah, 20 = paling kuat)\n",
                "    \"\"\"\n",
                "    try:\n",
                "        sf = Stockfish(path=stockfish_path)\n",
                "        sf.set_skill_level(skill_level)\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error loading Stockfish: {e}\")\n",
                "        return None\n",
                "    \n",
                "    model.eval()\n",
                "    results = {'wins': 0, 'draws': 0, 'losses': 0}\n",
                "    \n",
                "    for game_idx in range(n_games):\n",
                "        board = chess.Board()\n",
                "        sf.set_fen_position(board.fen())\n",
                "        \n",
                "        ai_is_white = (game_idx % 2 == 0)\n",
                "        move_count = 0\n",
                "        \n",
                "        while not board.is_game_over() and move_count < 200:\n",
                "            if board.turn == ai_is_white:\n",
                "                # AI's turn\n",
                "                state = env.encode_state()\n",
                "                env.board = board.copy()\n",
                "                state = env.encode_state()\n",
                "                legal_mask = env.get_legal_action_mask()\n",
                "                \n",
                "                with torch.no_grad():\n",
                "                    state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "                    mask_t = torch.BoolTensor(legal_mask).unsqueeze(0).to(device)\n",
                "                    log_probs, _ = model(state_t, mask_t)\n",
                "                    action = torch.argmax(log_probs, dim=-1).item()\n",
                "                \n",
                "                if action in env.action_to_move:\n",
                "                    move = env.action_to_move[action]\n",
                "                    if move in board.legal_moves:\n",
                "                        board.push(move)\n",
                "                    else:\n",
                "                        # Fallback: random legal move\n",
                "                        legal_moves = list(board.legal_moves)\n",
                "                        if legal_moves:\n",
                "                            board.push(random.choice(legal_moves))\n",
                "                else:\n",
                "                    legal_moves = list(board.legal_moves)\n",
                "                    if legal_moves:\n",
                "                        board.push(random.choice(legal_moves))\n",
                "            else:\n",
                "                # Stockfish's turn\n",
                "                sf.set_fen_position(board.fen())\n",
                "                best_move = sf.get_best_move_time(int(time_limit * 1000))\n",
                "                if best_move:\n",
                "                    board.push(chess.Move.from_uci(best_move))\n",
                "                else:\n",
                "                    break\n",
                "            \n",
                "            move_count += 1\n",
                "        \n",
                "        result = board.result()\n",
                "        if result == '1-0':\n",
                "            if ai_is_white:\n",
                "                results['wins'] += 1\n",
                "            else:\n",
                "                results['losses'] += 1\n",
                "        elif result == '0-1':\n",
                "            if ai_is_white:\n",
                "                results['losses'] += 1\n",
                "            else:\n",
                "                results['wins'] += 1\n",
                "        else:\n",
                "            results['draws'] += 1\n",
                "        \n",
                "        print(f\"  Game {game_idx+1}: {result} ({'AI White' if ai_is_white else 'AI Black'})\")\n",
                "    \n",
                "    model.train()\n",
                "    return results\n",
                "\n",
                "print(\"‚úÖ Stockfish evaluation function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model dan evaluasi\n",
                "try:\n",
                "    checkpoint = torch.load('/content/chess_model_best.pt')\n",
                "    network.load_state_dict(checkpoint['network_state_dict'])\n",
                "    print(f\"‚úÖ Loaded best model (win_rate={checkpoint.get('best_win_rate', 'N/A')})\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è Using current model (no best checkpoint found)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìä EVALUATION VS STOCKFISH\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for skill in [0, 1, 3, 5]:\n",
                "    print(f\"\\nüéØ Stockfish Skill Level {skill}:\")\n",
                "    results = evaluate_vs_stockfish(network, skill_level=skill, n_games=10)\n",
                "    if results:\n",
                "        total = results['wins'] + results['draws'] + results['losses']\n",
                "        print(f\"   Results: W={results['wins']} D={results['draws']} L={results['losses']}\")\n",
                "        print(f\"   Win Rate: {results['wins']/total:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Demo: AI vs AI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import chess.svg\n",
                "from IPython.display import display, HTML, clear_output\n",
                "import time as time_module\n",
                "\n",
                "def play_demo(max_moves=60, delay=0.5):\n",
                "    state, _ = env.reset()\n",
                "    moves_played = []\n",
                "    \n",
                "    for i in range(max_moves):\n",
                "        if env.board.is_game_over():\n",
                "            break\n",
                "        \n",
                "        clear_output(wait=True)\n",
                "        display(HTML(chess.svg.board(env.board, size=400)))\n",
                "        print(f\"Move {i+1}: {'White' if env.board.turn else 'Black'}\")\n",
                "        if moves_played:\n",
                "            print(f\"Last moves: {' '.join(moves_played[-6:])}\")\n",
                "        \n",
                "        legal_mask = env.get_legal_action_mask()\n",
                "        \n",
                "        # Use network to select move\n",
                "        with torch.no_grad():\n",
                "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "            mask_t = torch.BoolTensor(legal_mask).unsqueeze(0).to(device)\n",
                "            log_probs, value = network(state_t, mask_t)\n",
                "            action = torch.argmax(log_probs, dim=-1).item()\n",
                "            print(f\"Eval: {value.item():.3f}\")\n",
                "        \n",
                "        if action in env.action_to_move:\n",
                "            move = env.action_to_move[action]\n",
                "            moves_played.append(move.uci())\n",
                "        \n",
                "        state, _, done, _, _ = env.step(action)\n",
                "        time_module.sleep(delay)\n",
                "    \n",
                "    clear_output(wait=True)\n",
                "    display(HTML(chess.svg.board(env.board, size=400)))\n",
                "    print(f\"üèÜ Game Over! Result: {env.board.result()}\")\n",
                "    print(f\"Total moves: {len(moves_played)}\")\n",
                "    print(f\"Moves: {' '.join(moves_played)}\")\n",
                "\n",
                "play_demo()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Save Final Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model\n",
                "torch.save({\n",
                "    'network_state_dict': network.state_dict(),\n",
                "    'config': CONFIG,\n",
                "    'history': history,\n",
                "    'total_updates': CONFIG['total_updates'],\n",
                "}, '/content/chess_model_final.pt')\n",
                "\n",
                "print(\"‚úÖ Final model saved to /content/chess_model_final.pt\")\n",
                "\n",
                "# Download model\n",
                "from google.colab import files\n",
                "files.download('/content/chess_model_final.pt')\n",
                "\n",
                "# Download best model jika ada\n",
                "try:\n",
                "    files.download('/content/chess_model_best.pt')\n",
                "    print(\"‚úÖ Best model downloaded!\")\n",
                "except:\n",
                "    pass\n",
                "\n",
                "# Download training curves\n",
                "try:\n",
                "    files.download('/content/training_curves.png')\n",
                "except:\n",
                "    pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Tips untuk Training Lebih Lanjut:\n",
                "\n",
                "1. **Jalankan multiple sessions** - Resume dari checkpoint untuk training lebih lama\n",
                "2. **Tingkatkan `total_updates`** ke 10000-50000 untuk hasil lebih baik\n",
                "3. **Gunakan self-play** - Buat agent bermain melawan dirinya sendiri\n",
                "4. **Fine-tune melawan Stockfish** - Setelah bagus vs random, train vs Stockfish level rendah\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
